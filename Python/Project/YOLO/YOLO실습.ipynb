{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc8b2eab",
   "metadata": {},
   "source": [
    "## 📚 1단계: 패키지 설치 확인 및 환경 점검\n",
    "\n",
    "### 🎯 학습 목표\n",
    "여러분이 오늘 YOLO 실습을 진행하기 위해 필요한 모든 도구들이 제대로 설치되어 있는지 확인하겠습니다. 마치 요리를 시작하기 전에 재료와 도구를 점검하는 것처럼, AI 프로그래밍도 환경 설정이 가장 중요합니다.\n",
    "\n",
    "### 🔧 필요한 도구들 (라이브러리)\n",
    "\n",
    "**PyTorch** 🔥\n",
    "- AI의 뇌 역할을 하는 딥러닝 프레임워크입니다\n",
    "- YOLO가 실제로 동작하는 핵심 엔진이라고 생각하면 됩니다\n",
    "- GPU 가속을 통해 빠른 연산이 가능합니다\n",
    "\n",
    "**OpenCV** 📹\n",
    "- 컴퓨터 비전의 스위스 아미 나이프라고 불리는 라이브러리입니다\n",
    "- 웹캠에서 영상을 가져오고, 결과를 화면에 표시하는 역할을 합니다\n",
    "- 이미지 처리의 모든 기본 기능들이 들어있습니다\n",
    "\n",
    "**Ultralytics YOLO** 🎯\n",
    "- 복잡한 YOLO 알고리즘을 쉽게 사용할 수 있게 해주는 패키지입니다\n",
    "- 마치 자동차의 자동변속기처럼, 복잡한 내부 동작을 간단한 명령으로 제어할 수 있습니다\n",
    "\n",
    "### 💡 왜 이런 점검이 필요한가?\n",
    "\n",
    "프로그래밍에서 환경 설정 문제는 가장 흔하면서도 해결하기 까다로운 문제입니다. 특히 AI 관련 라이브러리들은 서로 복잡하게 연결되어 있어서, 하나라도 제대로 설치되지 않으면 전체가 작동하지 않을 수 있습니다.\n",
    "\n",
    "이 단계에서 모든 것이 정상적으로 출력된다면, 여러분은 이미 AI 개발자로서 첫 번째 관문을 통과한 것입니다! 🎉\n",
    "\n",
    "### 🚨 문제가 발생한다면?\n",
    "\n",
    "만약 어떤 패키지에서 오류가 발생한다면 당황하지 마세요. 이는 매우 정상적인 일이며, 실제 개발 현장에서도 자주 마주치는 상황입니다. 오류 메시지를 잘 읽어보고 선생님께 알려주시면 함께 해결해보겠습니다.\n",
    "\n",
    "### 🎓 실전 팁\n",
    "\n",
    "실제 AI 개발 프로젝트에서는 이런 환경 설정이 전체 프로젝트 시간의 30% 이상을 차지하기도 합니다. 그만큼 중요하고 기본적인 과정이니, 지금 배우는 것이 나중에 큰 도움이 될 것입니다.\n",
    "\n",
    "이제 셀을 실행해보고, 모든 체크마크(✅)가 나오는지 확인해보세요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b711eef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1단계: 필요한 라이브러리들을 불러와서 설치 상태 확인\n",
    "# =============================================================================\n",
    "# 필요한 라이브러리들을 하나씩 불러와서 제대로 설치되었는지 확인함\n",
    "# try-except 구문을 사용해서 오류가 발생해도 프로그램이 멈추지 않게 함\n",
    "\n",
    "# PyTorch 라이브러리 확인\n",
    "try:\n",
    "   # try 블록: 오류가 발생할 수 있는 코드를 안전하게 실행하는 구문\n",
    "   # 라이브러리 import가 실패할 수 있으므로 try로 감쌈\n",
    "   import torch\n",
    "   # PyTorch 라이브러리를 불러옴 (딥러닝의 핵심 엔진)\n",
    "   # PyTorch는 Facebook(Meta)에서 개발한 딥러닝 프레임워크\n",
    "   # 텐서 연산, 자동 미분, 신경망 구축 등의 기능 제공\n",
    "   # GPU 가속 연산을 지원해서 AI 모델 학습과 추론에 필수적\n",
    "   \n",
    "   print(\"✅ PyTorch 설치 완료!\")\n",
    "   # 성공시 체크마크(✅) 이모지와 함께 확인 메시지 출력\n",
    "   # 이모지를 사용해서 결과를 시각적으로 명확하게 표현\n",
    "   \n",
    "   print(f\"   버전: {torch.__version__}\")\n",
    "   # f-string을 사용해서 설치된 PyTorch 버전 정보를 동적으로 표시\n",
    "   # torch.__version__은 현재 설치된 PyTorch의 버전 문자열\n",
    "   # 들여쓰기로 하위 정보임을 표현해서 가독성 향상\n",
    "   # 버전 정보가 중요한 이유: 호환성 문제 확인과 기능 지원 여부 판단\n",
    "   \n",
    "except ImportError:\n",
    "   # ImportError: 모듈을 찾을 수 없거나 import에 실패했을 때 발생하는 예외\n",
    "   # 라이브러리가 설치되지 않았거나 설치에 문제가 있을 때 발생\n",
    "   # 파이썬 환경 설정 문제나 패키지 경로 문제도 이 오류 원인\n",
    "   print(\"❌ PyTorch가 설치되지 않았습니다!\")\n",
    "   # 실패시 X마크(❌) 이모지와 함께 명확한 오류 메시지 출력\n",
    "   # 사용자가 어떤 라이브러리에 문제가 있는지 즉시 파악 가능\n",
    "\n",
    "# OpenCV 라이브러리 확인 (컴퓨터 비전 처리용)\n",
    "try:\n",
    "   import cv2\n",
    "   # OpenCV 라이브러리를 불러옴 (컴퓨터 비전의 표준 라이브러리)\n",
    "   # cv2는 OpenCV의 파이썬 바인딩 모듈명\n",
    "   # 웹캠 연결, 이미지/영상 처리, 객체 검출 등의 기능 제공\n",
    "   # YOLO와 함께 사용해서 실시간 영상에서 객체 인식 수행\n",
    "   # 이미지 전처리, 후처리, 시각화 등에 필수적\n",
    "   \n",
    "   print(\"✅ OpenCV 설치 완료!\")\n",
    "   # 성공적인 OpenCV 설치 확인 메시지\n",
    "   \n",
    "   print(f\"   버전: {cv2.__version__}\")\n",
    "   # cv2.__version__으로 OpenCV 버전 정보 출력\n",
    "   # OpenCV 버전에 따라 지원하는 기능과 API가 다를 수 있음\n",
    "   # 최신 버전일수록 더 많은 AI 모델과 최적화 기능 지원\n",
    "   \n",
    "except ImportError:\n",
    "   # OpenCV import 실패 시 처리\n",
    "   # pip install opencv-python 명령으로 설치 가능\n",
    "   print(\"❌ OpenCV가 설치되지 않았습니다!\")\n",
    "   # 설치 실패 또는 누락을 명확히 알려줌\n",
    "\n",
    "# Ultralytics YOLO 라이브러리 확인 (객체 인식 AI 모델용)\n",
    "try:\n",
    "   from ultralytics import YOLO\n",
    "   # YOLO 클래스를 ultralytics 패키지에서 가져옴\n",
    "   # ultralytics는 YOLOv8을 포함한 최신 YOLO 모델들을 제공하는 패키지\n",
    "   # YOLO: You Only Look Once - 실시간 객체 검출에 특화된 AI 모델\n",
    "   # 한 번의 forward pass로 이미지 내 모든 객체를 동시에 검출\n",
    "   # 속도와 정확도의 균형이 뛰어나서 실시간 응용에 최적\n",
    "   \n",
    "   print(\"✅ Ultralytics YOLO 설치 완료!\")\n",
    "   # YOLO 사용 준비 완료를 알리는 메시지\n",
    "   # 이제 객체 인식 AI 모델을 사용할 수 있음\n",
    "   \n",
    "except ImportError:\n",
    "   # Ultralytics 패키지 import 실패 시 처리\n",
    "   # pip install ultralytics 명령으로 설치 가능\n",
    "   print(\"❌ Ultralytics가 설치되지 않았습니다!\")\n",
    "   # 설치 필요성을 명확히 알려주는 메시지\n",
    "\n",
    "# 구분선 출력 (결과를 보기 좋게 정리)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "# \\n: 줄바꿈 문자로 한 줄 띄워서 구분선과 위 내용 사이에 여백 생성\n",
    "# \"=\"*50: = 문자를 50번 반복해서 긴 구분선 생성\n",
    "# 시각적으로 라이브러리 체크 섹션의 끝을 명확히 표시\n",
    "# 결과 요약이나 다음 단계로 넘어감을 알리는 구분 효과\n",
    "\n",
    "# 💡 이 코드 블록의 교육적 목적과 중요성:\n",
    "# 1. 실습 환경 사전 점검: 필요한 모든 라이브러리가 준비되었는지 확인\n",
    "# 2. 예외 처리 학습: try-except 구문의 실용적 사용 사례 제시\n",
    "# 3. 디버깅 지원: 문제 발생 시 어떤 라이브러리가 누락되었는지 명확히 파악\n",
    "# 4. 버전 호환성 확인: 각 라이브러리 버전 정보로 호환성 문제 사전 방지\n",
    "# 5. 사용자 친화적 피드백: 이모지와 명확한 메시지로 상태를 직관적으로 전달\n",
    "# 6. 프로그래밍 모범 사례: 안전한 코딩과 오류 처리의 좋은 예시\n",
    "# 7. 실무 적용: 실제 프로젝트에서 자주 사용되는 환경 검증 패턴\n",
    "\n",
    "# 📋 다음 단계 진행 조건:\n",
    "# - 모든 라이브러리에서 ✅ 표시가 나와야 함\n",
    "# - ❌가 하나라도 있으면 해당 라이브러리를 먼저 설치해야 함\n",
    "# - 설치 명령어: pip install torch opencv-python ultralytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d71e3c",
   "metadata": {},
   "source": [
    "## 🖥️ 2단계: GPU 환경 확인하기\n",
    "\n",
    "### 🎯 학습 목표\n",
    "우리 컴퓨터에 탑재된 GPU가 제대로 인식되고 있는지 확인해보겠습니다. GPU는 AI 연산을 엄청나게 빠르게 처리해주는 핵심 하드웨어입니다.\n",
    "\n",
    "### 💎 GPU가 중요한 이유\n",
    "\n",
    "**속도의 차이** ⚡\n",
    "- CPU만 사용할 때: 이미지 1장당 2-5초 소요\n",
    "- GPU를 사용할 때: 이미지 1장당 0.1-0.3초 소요\n",
    "- 약 10-50배의 성능 차이가 납니다!\n",
    "\n",
    "**실시간 처리의 핵심** 🎬\n",
    "- 웹캠 영상은 초당 30프레임이 들어옵니다\n",
    "- CPU만으로는 실시간 처리가 거의 불가능합니다\n",
    "- GPU가 있어야 진짜 \"실시간\" 객체 인식이 가능합니다\n",
    "\n",
    "### 🔍 확인할 내용들\n",
    "\n",
    "**CUDA 지원 여부** 🚀\n",
    "- CUDA는 NVIDIA GPU에서 AI 연산을 가속화하는 기술입니다\n",
    "- PyTorch가 CUDA를 인식하면 GPU 사용이 가능합니다\n",
    "\n",
    "**GPU 개수와 정보** 📊\n",
    "- 우리 컴퓨터에는 듀얼 GPU가 탑재되어 있습니다\n",
    "- 각 GPU의 이름과 메모리 용량을 확인할 수 있습니다\n",
    "- GPU마다 다른 작업을 할당해서 더욱 빠른 처리가 가능합니다\n",
    "\n",
    "### 💡 듀얼 GPU의 장점\n",
    "\n",
    "**병렬 처리** 🔄\n",
    "- 두 개의 GPU를 동시에 사용해서 작업을 나누어 처리\n",
    "- 예: GPU 0에서는 빠른 모델, GPU 1에서는 정확한 모델 실행\n",
    "\n",
    "**메모리 확장** 💾\n",
    "- 각 GPU마다 독립적인 메모리를 가지고 있음\n",
    "- 더 큰 모델이나 고해상도 영상 처리 가능\n",
    "\n",
    "### 🎓 실전에서는?\n",
    "\n",
    "실제 AI 서비스 회사에서는 훨씬 더 강력한 GPU들을 사용합니다. NVIDIA A100, H100 같은 전문 AI 칩들이 수십 개씩 연결된 서버를 사용하죠. 오늘 여러분이 경험하는 듀얼 GPU 환경도 상당히 고급 사양입니다!\n",
    "\n",
    "### 🚨 만약 GPU가 인식되지 않는다면?\n",
    "\n",
    "걱정하지 마세요! CPU만으로도 학습은 충분히 가능합니다. 다만 속도가 조금 느릴 뿐입니다. 실제로 많은 개발자들이 개발 초기에는 CPU로 테스트하고, 나중에 GPU 서버에서 본격 실행하는 방식을 사용합니다.\n",
    "\n",
    "이제 셀을 실행해서 우리 컴퓨터의 GPU 파워를 확인해보세요! 🔥\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b0ed10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2단계: GPU 환경 확인하기\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "두 번째 단계: 우리 컴퓨터의 GPU가 제대로 인식되는지 확인해봅시다!\n",
    "GPU가 있으면 훨씬 빠른 처리가 가능합니다.\n",
    "\"\"\"\n",
    "# 독스트링(docstring)으로 이 코드 블록의 목적을 명확히 설명함\n",
    "# GPU: Graphics Processing Unit - 그래픽 처리 장치\n",
    "# 원래는 그래픽 렌더링용이지만 병렬 연산에 뛰어나서 AI 연산에 활용\n",
    "# CPU보다 수십~수백 배 빠른 딥러닝 모델 처리 가능\n",
    "\n",
    "# 하드웨어 정보 확인 섹션 시작 알림\n",
    "print(\"🖥️ 컴퓨터 하드웨어 정보\")\n",
    "# 🖥️ 컴퓨터 이모지로 하드웨어 정보 확인 단계임을 직관적으로 표현\n",
    "# 사용자에게 현재 시스템의 AI 처리 능력을 점검 중임을 알림\n",
    "\n",
    "print(\"=\"*30)\n",
    "# = 문자 30개로 구분선을 만들어서 섹션을 시각적으로 구분\n",
    "# 이전 라이브러리 체크와 현재 하드웨어 체크를 명확히 분리\n",
    "\n",
    "# CUDA (GPU) 사용 가능 여부 확인\n",
    "if torch.cuda.is_available():\n",
    "   # torch.cuda.is_available(): PyTorch가 CUDA를 통해 GPU를 사용할 수 있는지 확인\n",
    "   # CUDA: NVIDIA에서 개발한 GPU 병렬 컴퓨팅 플랫폼\n",
    "   # True 반환 조건:\n",
    "   #   1. NVIDIA GPU가 컴퓨터에 설치되어 있음\n",
    "   #   2. CUDA 드라이버가 올바르게 설치됨\n",
    "   #   3. PyTorch가 CUDA 지원 버전으로 설치됨\n",
    "   #   4. GPU가 CUDA 호환 모델임 (Compute Capability 3.5 이상)\n",
    "   \n",
    "   print(\"🚀 GPU 가속 사용 가능!\")\n",
    "   # 🚀 로켓 이모지로 빠른 처리 속도를 암시\n",
    "   # GPU 가속: CPU 대비 10~100배 빠른 AI 모델 연산 가능\n",
    "   # 특히 YOLO 같은 실시간 객체 검출에서 GPU는 필수적\n",
    "   \n",
    "   print(f\"🔢 사용 가능한 GPU 개수: {torch.cuda.device_count()}\")\n",
    "   # torch.cuda.device_count(): 시스템에서 PyTorch가 인식한 GPU 총 개수\n",
    "   # 🔢 숫자 이모지로 개수 정보임을 표현\n",
    "   # 듀얼 GPU, 쿼드 GPU 시스템에서는 2개, 4개 등으로 표시됨\n",
    "   # 개수가 많을수록 더 큰 모델이나 배치 처리 가능\n",
    "   \n",
    "   # 각 GPU의 상세 정보를 하나씩 출력하는 반복문\n",
    "   for i in range(torch.cuda.device_count()):\n",
    "       # range(torch.cuda.device_count()): 0부터 GPU 개수-1까지 반복\n",
    "       # 예: GPU가 2개면 i는 0, 1 순서로 반복\n",
    "       # 각 GPU를 개별적으로 식별하고 정보를 표시\n",
    "       \n",
    "       gpu_name = torch.cuda.get_device_name(i)\n",
    "       # torch.cuda.get_device_name(i): i번째 GPU의 모델명을 가져옴\n",
    "       # 예: \"NVIDIA GeForce RTX 4090\", \"NVIDIA Tesla V100\" 등\n",
    "       # GPU 모델명으로 성능과 메모리 용량을 대략 파악 가능\n",
    "       \n",
    "       print(f\"   GPU {i}: {gpu_name}\")\n",
    "       # f-string으로 GPU 번호와 모델명을 동적으로 출력\n",
    "       # 들여쓰기 3칸으로 하위 정보임을 시각적으로 표현\n",
    "       # 사용자가 어떤 GPU가 사용 가능한지 명확히 파악 가능\n",
    "       \n",
    "       # GPU 메모리 정보 계산 및 출력\n",
    "       memory_total = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "       # torch.cuda.get_device_properties(i): i번째 GPU의 상세 속성 정보 반환\n",
    "       # .total_memory: GPU 메모리 총 용량 (바이트 단위)\n",
    "       # / 1024**3: 바이트를 기가바이트로 변환\n",
    "       #   1024 bytes = 1 KB\n",
    "       #   1024 KB = 1 MB  \n",
    "       #   1024 MB = 1 GB\n",
    "       #   따라서 1024³ = 1,073,741,824로 나누면 GB 단위\n",
    "       \n",
    "       print(f\"   메모리: {memory_total:.1f} GB\")\n",
    "       # :.1f 포맷으로 소수점 첫째자리까지 표시 (예: 24.0 GB)\n",
    "       # GPU 메모리는 AI 모델 크기의 한계를 결정하는 중요한 요소\n",
    "       # 더 큰 메모리일수록 더 큰 모델이나 더 많은 배치 처리 가능\n",
    "       # YOLO 모델 실행에는 최소 4GB 이상 권장\n",
    "   \n",
    "   # 현재 기본으로 사용 중인 GPU 번호 확인\n",
    "   current_device = torch.cuda.current_device()\n",
    "   # torch.cuda.current_device(): 현재 PyTorch가 기본으로 사용하는 GPU 번호\n",
    "   # 멀티 GPU 환경에서 어떤 GPU가 메인으로 설정되었는지 확인\n",
    "   # 보통 GPU 0이 기본값이지만 설정에 따라 달라질 수 있음\n",
    "   \n",
    "   print(f\"🎯 현재 사용 중인 GPU: {current_device}\")\n",
    "   # 🎯 타겟 이모지로 현재 선택된 GPU를 강조\n",
    "   # 사용자가 어떤 GPU에서 모델이 실행될지 미리 파악 가능\n",
    "   # 필요시 torch.cuda.set_device()로 다른 GPU로 변경 가능\n",
    "\n",
    "else:\n",
    "   # GPU를 사용할 수 없는 경우의 처리\n",
    "   # 발생 가능한 원인들:\n",
    "   #   1. NVIDIA GPU가 없음 (Intel/AMD GPU는 CUDA 미지원)\n",
    "   #   2. CUDA 드라이버가 설치되지 않음\n",
    "   #   3. PyTorch가 CPU 전용 버전으로 설치됨\n",
    "   #   4. GPU가 너무 오래되어 CUDA 미지원\n",
    "   #   5. 드라이버와 CUDA 버전 불일치\n",
    "   \n",
    "   print(\"⚠️ GPU를 사용할 수 없습니다. CPU 모드로 진행됩니다.\")\n",
    "   # ⚠️ 경고 이모지로 주의사항임을 표시\n",
    "   # CPU 모드: GPU 없이 CPU만으로 AI 모델 실행\n",
    "   # 기능은 동일하지만 처리 속도가 현저히 느려짐\n",
    "   \n",
    "   print(\"   (속도가 조금 느릴 수 있어요)\")\n",
    "   # 친근한 톤으로 CPU 사용시 예상되는 성능 저하를 미리 안내\n",
    "   # \"조금 느릴\"이라고 표현했지만 실제로는 상당한 차이가 날 수 있음\n",
    "   # 사용자의 기대치를 적절히 조정하는 역할\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "# \\n으로 줄바꿈을 추가해서 다음 섹션과의 여백 확보\n",
    "# = 문자 50개로 이전보다 더 긴 구분선 생성\n",
    "# 하드웨어 체크 섹션의 완료를 명확히 표시\n",
    "\n",
    "# 💡 이 코드 블록의 교육적 의미와 실용적 가치:\n",
    "# 1. 시스템 환경 파악: AI 개발 전 필수적인 하드웨어 사양 확인\n",
    "# 2. 성능 예측: GPU 유무와 사양으로 실행 속도 미리 예상 가능\n",
    "# 3. 디버깅 지원: GPU 관련 문제 발생 시 원인 파악의 출발점\n",
    "# 4. 최적화 기준: 하드웨어에 맞는 모델 크기나 배치 크기 결정 근거\n",
    "# 5. 사용자 경험: 명확한 정보 제공으로 사용자 불안감 해소\n",
    "# 6. 멀티 GPU 관리: 여러 GPU 환경에서 자원 현황 파악\n",
    "\n",
    "# 🔥 실제 출력 예시들:\n",
    "# GPU가 있는 경우:\n",
    "#   🚀 GPU 가속 사용 가능!\n",
    "#   🔢 사용 가능한 GPU 개수: 2\n",
    "#      GPU 0: Quadro RTX 5000\n",
    "#      메모리: 16.0 GB\n",
    "#      GPU 1: Quadro RTX 5000\n",
    "#      메모리: 16.0 GB\n",
    "#   🎯 현재 사용 중인 GPU: 0\n",
    "#\n",
    "# GPU가 없는 경우:\n",
    "#   ⚠️ GPU를 사용할 수 없습니다. CPU 모드로 진행됩니다.\n",
    "#      (속도가 조금 느릴 수 있어요)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3799388d",
   "metadata": {},
   "source": [
    "## 🤖 3단계: YOLO AI 모델 불러오기\n",
    "\n",
    "### 🎯 학습 목표\n",
    "이제 본격적으로 AI 모델을 컴퓨터에 불러와서 사용할 준비를 해보겠습니다. YOLO 모델은 이미 수백만 장의 이미지로 학습이 완료된 똑똑한 AI입니다.\n",
    "\n",
    "### 🧠 YOLO 모델의 특징\n",
    "\n",
    "**사전 훈련된 모델** 📚\n",
    "- Microsoft COCO 데이터셋으로 훈련되었습니다\n",
    "- 일상생활에서 볼 수 있는 80가지 객체를 인식할 수 있습니다\n",
    "- 사람, 자동차, 고양이, 휴대폰 등 다양한 물체를 구분합니다\n",
    "\n",
    "**YOLOv8 버전들의 차이** ⚖️\n",
    "- **Nano (n)**: 가장 빠름, 정확도는 상대적으로 낮음\n",
    "- **Small (s)**: 속도와 정확도의 균형\n",
    "- **Medium (m)**: 높은 정확도, 조금 느림\n",
    "- **Large (l)**: 최고 정확도, 가장 느림\n",
    "\n",
    "### 🚀 GPU vs CPU 처리\n",
    "\n",
    "**GPU 사용시** ⚡\n",
    "- 모델이 GPU 메모리에 로드됩니다\n",
    "- 병렬 연산으로 초고속 처리가 가능합니다\n",
    "- 실시간 웹캠 처리에 최적화됩니다\n",
    "\n",
    "**CPU 사용시** 💻\n",
    "- 모델이 시스템 RAM에 로드됩니다\n",
    "- 순차적 연산으로 처리 속도가 느립니다\n",
    "- 배치 처리나 정적 이미지 분석에 적합합니다\n",
    "\n",
    "### 🎯 80가지 인식 가능한 객체들\n",
    "\n",
    "YOLO가 인식할 수 있는 주요 객체들:\n",
    "- **사람과 동물**: person, cat, dog, horse, sheep, cow, bird, etc.\n",
    "- **교통수단**: car, bus, truck, bicycle, motorcycle, train, boat, etc.\n",
    "- **일상용품**: chair, table, laptop, cell phone, book, etc.\n",
    "- **음식**: apple, banana, pizza, hot dog, cake, etc.\n",
    "\n",
    "### 💡 모델 로딩 과정\n",
    "\n",
    "**첫 실행시** 📥\n",
    "- 인터넷에서 모델 파일(.pt)을 자동 다운로드합니다\n",
    "- 약 6MB 정도의 작은 파일입니다 (nano 버전 기준)\n",
    "- 다운로드된 파일은 로컬에 저장되어 재사용됩니다\n",
    "\n",
    "**재실행시** ⚡\n",
    "- 이미 다운로드된 파일을 바로 로드합니다\n",
    "- 몇 초 안에 모델 준비가 완료됩니다\n",
    "\n",
    "### 🔧 device 설정의 중요성\n",
    "\n",
    "```python\n",
    "model.to(device)  # 이 한 줄이 성능을 결정합니다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02c5e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3단계: 첫 번째 YOLO AI 모델 불러오기\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "세 번째 단계: 우리의 첫 번째 AI 모델을 불러와봅시다!\n",
    "YOLO는 이미 학습이 완료된 똑똑한 AI입니다.\n",
    "\"\"\"\n",
    "# 독스트링으로 이 단계의 의미를 명확히 설명함\n",
    "# YOLO: You Only Look Once - 한 번만 보면 모든 객체를 찾아내는 AI\n",
    "# 이미 수백만 장의 이미지로 훈련된 사전 학습 모델 사용\n",
    "# 별도 학습 없이 바로 80가지 객체를 인식할 수 있는 완성된 AI\n",
    "\n",
    "# 진행 상황을 알려주는 제목 출력\n",
    "print(\"🤖 YOLO 모델 준비하기\")\n",
    "# 🤖 로봇 이모지로 AI 모델을 직관적으로 표현\n",
    "# 사용자에게 인공지능 모델을 다루는 단계임을 명확히 알림\n",
    "\n",
    "print(\"=\"*25)\n",
    "# = 문자 25개로 구분선을 만들어서 이전 섹션과 시각적으로 분리\n",
    "# 각 단계별로 다른 길이의 구분선 사용해서 구조적 계층 표현\n",
    "\n",
    "# YOLO 모델 다운로드 및 로드\n",
    "print(\"📥 YOLOv8 nano 모델 다운로드 중...\")\n",
    "# 📥 다운로드 이모지로 인터넷에서 파일을 받고 있음을 표현\n",
    "# 사용자에게 다운로드 과정이 진행 중임을 미리 알려서 대기하도록 안내\n",
    "\n",
    "model = YOLO('yolov8n.pt')\n",
    "# YOLO() 클래스로 모델 인스턴스 생성\n",
    "# 'yolov8n.pt': YOLOv8 nano 버전의 사전 훈련된 모델 파일\n",
    "#   - 'yolov8': YOLO 버전 8 (2023년 출시된 최신 버전)\n",
    "#   - 'n': nano - 가장 작고 빠른 모델 (모바일/임베디드 환경용)\n",
    "#   - '.pt': PyTorch 모델 파일 확장자\n",
    "# 모델 파일이 로컬에 없으면 자동으로 Ultralytics 서버에서 다운로드\n",
    "# 첫 실행시에만 다운로드하고 이후에는 캐시된 파일 사용\n",
    "\n",
    "# YOLOv8 모델 크기별 특성:\n",
    "#   nano (n): 가장 빠름, 정확도 낮음, 모델 크기 6MB\n",
    "#   small (s): 빠름, 정확도 보통, 모델 크기 22MB  \n",
    "#   medium (m): 보통, 정확도 높음, 모델 크기 52MB\n",
    "#   large (l): 느림, 정확도 매우 높음, 모델 크기 131MB\n",
    "#   extra-large (x): 가장 느림, 정확도 최고, 모델 크기 218MB\n",
    "\n",
    "# GPU 사용 가능 여부에 따라 처리 장치 결정\n",
    "if torch.cuda.is_available():\n",
    "   # GPU가 사용 가능한 경우의 최적화 설정\n",
    "   device = 'cuda:0'\n",
    "   # 'cuda:0': 첫 번째 NVIDIA GPU 지정\n",
    "   # 멀티 GPU 환경에서는 'cuda:1', 'cuda:2' 등으로 다른 GPU 선택 가능\n",
    "   # 'cuda'만 쓰면 기본 GPU 사용, 번호 지정으로 특정 GPU 선택\n",
    "   \n",
    "   model.to(device)\n",
    "   # .to(device): 모델의 모든 파라미터와 버퍼를 지정된 장치로 이동\n",
    "   # CPU 메모리에서 GPU 메모리로 모델 전체를 복사\n",
    "   # 이후 모든 연산이 GPU에서 수행되어 대폭적인 속도 향상\n",
    "   # GPU 메모리 사용량: 모델 크기 + 입력 데이터 + 중간 계산 결과\n",
    "   \n",
    "   print(f\"🚀 모델을 {device}로 이동했습니다!\")\n",
    "   # 🚀 로켓 이모지로 GPU 가속의 빠른 속도를 암시\n",
    "   # f-string으로 실제 사용되는 GPU 장치명을 동적으로 표시\n",
    "   # 사용자에게 모델이 GPU에서 실행될 것임을 확신시킴\n",
    "\n",
    "else:\n",
    "   # GPU가 없거나 사용할 수 없는 경우의 대안 설정\n",
    "   device = 'cpu'\n",
    "   # 'cpu': 중앙처리장치에서 모델 실행\n",
    "   # GPU 대비 느리지만 모든 컴퓨터에서 동작 보장\n",
    "   # 메모리 사용량은 GPU보다 낮지만 처리 속도가 10~100배 느림\n",
    "   \n",
    "   print(\"💻 CPU 모드로 실행합니다!\")\n",
    "   # 💻 노트북 이모지로 일반적인 컴퓨터 처리를 표현\n",
    "   # GPU 없는 환경에서도 실행 가능함을 안심시키는 메시지\n",
    "\n",
    "# 모델이 학습된 객체 클래스들 정보 확인\n",
    "print(f\"\\n🎯 이 AI가 인식할 수 있는 객체는 {len(model.names)}가지입니다:\")\n",
    "# model.names: 모델이 인식할 수 있는 객체 클래스들의 딕셔너리\n",
    "# 형태: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', ...}\n",
    "# len(model.names): 딕셔너리의 키 개수 = 인식 가능한 객체 종류 수\n",
    "# YOLOv8은 COCO 데이터셋으로 훈련되어 80가지 객체 클래스 인식 가능\n",
    "# 🎯 타겟 이모지로 AI의 인식 대상을 표현\n",
    "\n",
    "# COCO 데이터셋의 80가지 객체 클래스:\n",
    "# 사람, 동물 (person, cat, dog, horse, sheep, cow, elephant, bear, zebra, giraffe)\n",
    "# 교통수단 (bicycle, car, motorcycle, airplane, bus, train, truck, boat)\n",
    "# 음식 (apple, banana, sandwich, orange, broccoli, carrot, pizza, donut, cake)\n",
    "# 일상용품 (chair, couch, bed, dining table, toilet, tv, laptop, mouse, remote, keyboard)\n",
    "# 스포츠용품 (baseball bat, baseball glove, skateboard, surfboard, tennis racket)\n",
    "# 기타 (traffic light, fire hydrant, stop sign, parking meter, bench, bird, etc.)\n",
    "\n",
    "print(\"주요 객체들:\", list(model.names.values())[:10], \"...\")\n",
    "# model.names.values(): 딕셔너리의 모든 값(객체 이름들)을 가져옴\n",
    "# list(): 딕셔너리 값들을 리스트로 변환\n",
    "# [:10]: 처음 10개만 슬라이싱해서 미리보기로 표시\n",
    "# \"...\": 더 많은 객체가 있음을 암시하는 생략 표시\n",
    "# 전체 80개를 다 출력하면 너무 길어서 주요한 것들만 보여줌\n",
    "\n",
    "print(\"✅ 모델 준비 완료!\")\n",
    "# ✅ 체크마크로 성공적인 모델 로딩 완료를 표시\n",
    "# 이제 실제 객체 인식을 수행할 준비가 완전히 끝났음을 알림\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "# \\n으로 줄바꿈을 추가해서 다음 섹션과의 시각적 분리\n",
    "# = 문자 50개로 이전보다 긴 구분선 생성\n",
    "# 모델 준비 단계의 완료를 명확히 표시\n",
    "\n",
    "# 💡 이 단계에서 일어나는 중요한 과정들:\n",
    "# 1. 모델 파일 다운로드: 인터넷에서 사전 훈련된 YOLO 모델 받기\n",
    "# 2. 모델 로딩: 파일을 메모리에 올려서 사용 가능한 상태로 만들기\n",
    "# 3. 장치 최적화: GPU 또는 CPU 중 최적의 처리 장치 선택\n",
    "# 4. 메모리 이동: 모델을 선택된 장치의 메모리로 이동\n",
    "# 5. 클래스 정보 확인: 인식 가능한 객체 종류 파악\n",
    "# 6. 준비 완료: 이제 실제 이미지에서 객체 검출 가능한 상태\n",
    "\n",
    "# 🔧 기술적 세부사항:\n",
    "# - 모델 크기: YOLOv8n은 약 6MB (다운로드 빠름, 메모리 사용량 적음)\n",
    "# - 추론 속도: GPU에서 실시간 처리 가능 (30fps 이상)\n",
    "# - 정확도: nano 버전이지만 일반적인 용도에는 충분한 성능\n",
    "# - 호환성: PyTorch 백엔드로 안정적이고 확장성 좋음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7dabc1",
   "metadata": {},
   "source": [
    "## 📸 4단계: 샘플 이미지로 첫 번째 테스트\n",
    "\n",
    "### 🎯 학습 목표\n",
    "웹캠을 사용하기 전에 먼저 정적인 이미지로 YOLO의 객체 인식 능력을 테스트해보겠습니다. 이는 AI 개발에서 매우 중요한 검증 단계입니다.\n",
    "\n",
    "### 🔍 왜 샘플 이미지부터 시작하나요?\n",
    "\n",
    "**안전한 테스트 환경** 🛡️\n",
    "- 웹캠보다 단순하고 예측 가능한 환경입니다\n",
    "- 문제가 발생해도 쉽게 원인을 파악할 수 있습니다\n",
    "- 모델의 기본 성능을 먼저 확인할 수 있습니다\n",
    "\n",
    "**결과 분석이 용이** 📊\n",
    "- 같은 이미지로 반복 테스트가 가능합니다\n",
    "- 다양한 설정값의 효과를 비교할 수 있습니다\n",
    "- 스크린샷을 찍어서 결과를 저장하기 쉽습니다\n",
    "\n",
    "### 🌐 인터넷에서 샘플 이미지 가져오기\n",
    "\n",
    "**Ultralytics 공식 샘플 이미지** 🖼️\n",
    "- `bus.jpg`: 다양한 객체가 포함된 거리 풍경\n",
    "- `zidane.jpg`: 스포츠 경기 장면 (사람 인식 테스트용)\n",
    "\n",
    "이 이미지들은 YOLO 성능을 보여주기 위해 특별히 선별된 것들입니다!\n",
    "\n",
    "### 🎨 결과 시각화의 마법\n",
    "\n",
    "**plot() 함수의 역할** ✨\n",
    "```python\n",
    "result_image = results[0].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5834c331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4단계: 샘플 이미지로 첫 번째 테스트\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "네 번째 단계: 샘플 이미지로 객체 인식을 테스트해봅시다!\n",
    "\"\"\"\n",
    "# 독스트링으로 이 단계의 목적을 명확히 설명함\n",
    "# 실제 이미지를 사용해서 YOLO 모델이 제대로 작동하는지 검증\n",
    "# 웹캠 사용 전에 정적 이미지로 먼저 테스트해서 안정성 확보\n",
    "\n",
    "# 이미지 처리와 시각화에 필요한 추가 라이브러리들 불러오기\n",
    "import numpy as np\n",
    "# NumPy: 수치 연산용 라이브러리\n",
    "# 이미지는 실제로 픽셀값들의 숫자 배열로 처리됨\n",
    "# 3차원 배열 형태: (높이, 너비, 색상채널)\n",
    "# OpenCV와 PIL에서 이미지를 numpy 배열로 다룸\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Matplotlib: 데이터 시각화 라이브러리\n",
    "# pyplot 모듈로 그래프와 이미지를 화면에 표시\n",
    "# YOLO 결과를 시각적으로 확인하는 데 필수적\n",
    "# Jupyter 노트북 환경에서 인라인 이미지 표시 지원\n",
    "\n",
    "from PIL import Image\n",
    "# PIL (Python Image Library): 이미지 파일 처리 전용 라이브러리\n",
    "# 다양한 이미지 형식 지원 (JPEG, PNG, GIF, BMP 등)\n",
    "# 이미지 열기, 저장, 변환, 크기 조정 등의 기능 제공\n",
    "# YOLO 모델 입력 형식으로 변환하는 데 사용\n",
    "\n",
    "import requests\n",
    "# Requests: HTTP 요청을 간단하게 처리하는 라이브러리\n",
    "# 인터넷에서 파일을 다운로드할 때 사용\n",
    "# GET, POST 등의 HTTP 메서드 지원\n",
    "# 웹 API나 온라인 리소스에 접근할 때 필수\n",
    "\n",
    "from io import BytesIO\n",
    "# BytesIO: 바이트 데이터를 파일처럼 다루게 해주는 도구\n",
    "# 메모리상의 바이트 데이터를 파일 객체처럼 조작 가능\n",
    "# 다운로드된 이미지 데이터를 PIL.Image로 열 때 사용\n",
    "# 임시 파일 생성 없이 메모리에서 직접 처리\n",
    "\n",
    "# 테스트 시작 알림\n",
    "print(\"📸 샘플 이미지 테스트\")\n",
    "# 📸 카메라 이모지로 이미지 처리 테스트임을 표현\n",
    "# 사용자에게 실제 객체 인식 테스트가 시작됨을 알림\n",
    "\n",
    "print(\"=\"*22)\n",
    "# = 문자 22개로 구분선을 만들어서 이전 섹션과 분리\n",
    "# 각 섹션마다 다른 길이의 구분선으로 시각적 계층 구조 표현\n",
    "\n",
    "# 테스트용 샘플 이미지 URL 목록 (Ultralytics 공식 샘플)\n",
    "sample_urls = [\n",
    "   \"https://ultralytics.com/images/bus.jpg\",\n",
    "   # 버스와 다양한 객체들이 있는 거리 사진\n",
    "   # 교통수단, 사람, 자동차 등 여러 객체가 포함된 복합적 장면\n",
    "   # YOLO 모델의 다중 객체 검출 능력을 테스트하기에 적합\n",
    "   # Ultralytics 공식 데모 이미지로 안정적인 다운로드 보장\n",
    "   \n",
    "   \"https://ultralytics.com/images/zidane.jpg\"\n",
    "   # 축구 경기 사진 (사람 인식 테스트용)\n",
    "   # 축구선수 지단의 유명한 사진으로 사람 검출 성능 확인\n",
    "   # 스포츠 장면에서의 인물 인식 정확도 테스트\n",
    "   # 단일 객체 집중 검출 시나리오 검증\n",
    "]\n",
    "# 공식 샘플 이미지를 사용하는 이유:\n",
    "# 1. 다운로드 안정성 보장 (서버 안정성)\n",
    "# 2. 적절한 해상도와 품질 (모델 테스트에 최적화)\n",
    "# 3. 다양한 시나리오 제공 (복합 장면 vs 단일 객체)\n",
    "# 4. 저작권 문제 없음 (공식 제공 자료)\n",
    "\n",
    "def download_and_test_image(url, title):\n",
    "   \"\"\"이미지를 다운로드하고 객체 인식 테스트하는 함수\"\"\"\n",
    "   # 함수 정의: 코드 재사용성과 구조화를 위해 별도 함수로 분리\n",
    "   # 매개변수:\n",
    "   #   url: 다운로드할 이미지의 인터넷 주소\n",
    "   #   title: 사용자에게 표시할 이미지 설명\n",
    "   \n",
    "   try:\n",
    "       # try 블록: 오류가 발생할 수 있는 코드를 안전하게 실행\n",
    "       # 네트워크 오류, 파일 형식 오류, 모델 처리 오류 등에 대비\n",
    "       \n",
    "       print(f\"🔄 {title} 처리 중...\")\n",
    "       # 🔄 새로고침 이모지로 처리 진행 중임을 표현\n",
    "       # f-string으로 현재 처리중인 이미지 제목을 동적으로 표시\n",
    "       # 사용자에게 진행 상황을 실시간으로 알려줌\n",
    "       \n",
    "       # 인터넷에서 이미지 다운로드하기\n",
    "       response = requests.get(url)\n",
    "       # requests.get(): HTTP GET 요청으로 지정된 URL에서 데이터 다운로드\n",
    "       # response 객체에 서버 응답 (상태코드, 헤더, 본문 데이터) 저장\n",
    "       # 이미지 파일의 바이너리 데이터가 response.content에 저장됨\n",
    "       \n",
    "       image = Image.open(BytesIO(response.content))\n",
    "       # BytesIO(): 다운로드된 바이트 데이터를 파일 객체처럼 변환\n",
    "       # Image.open(): PIL 라이브러리로 이미지 파일을 열어서 Image 객체 생성\n",
    "       # 메모리상에서 직접 처리하므로 임시 파일 생성 불필요\n",
    "       # 결과: PIL Image 객체 (YOLO 모델 입력 가능한 형태)\n",
    "       \n",
    "       # YOLO 모델로 객체 인식 실행\n",
    "       results = model(image, device=device, conf=0.5, iou=0.7)\n",
    "       # model(): YOLO 모델에 이미지를 입력해서 객체 검출 수행\n",
    "       # 매개변수 설명:\n",
    "       #   image: 인식할 대상 이미지 (PIL Image 또는 numpy 배열)\n",
    "       #   device: 연산을 수행할 장치 (cuda:0 또는 cpu)\n",
    "       #   conf=0.5: 신뢰도 임계값 (50% 이상 확신하는 객체만 검출)\n",
    "       #   iou=0.7: IoU 임계값 (겹치는 박스 제거를 위한 Non-Max Suppression)\n",
    "       \n",
    "       # 신뢰도(confidence): 모델이 해당 객체라고 확신하는 정도 (0~1)\n",
    "       # IoU(Intersection over Union): 박스 겹침 정도 측정 (중복 제거용)\n",
    "       \n",
    "       # 인식 결과를 시각적으로 표현한 이미지 생성\n",
    "       result_image = results[0].plot()\n",
    "       # results[0]: 첫 번째(유일한) 이미지의 인식 결과 객체\n",
    "       # .plot(): 원본 이미지에 검출 결과를 그린 새로운 이미지 생성\n",
    "       # 바운딩 박스(사각형), 클래스 라벨, 신뢰도 점수가 자동으로 표시됨\n",
    "       # 색상은 객체 클래스별로 자동 할당되어 구분하기 쉬움\n",
    "       \n",
    "       # matplotlib을 사용해서 결과를 화면에 표시\n",
    "       plt.figure(figsize=(12, 8))\n",
    "       # plt.figure(): 새로운 그래프 창 생성\n",
    "       # figsize=(12, 8): 가로 12인치, 세로 8인치 크기 설정\n",
    "       # 충분히 큰 크기로 설정해서 세부사항을 명확히 볼 수 있게 함\n",
    "       \n",
    "       plt.imshow(result_image)\n",
    "       # plt.imshow(): 이미지를 matplotlib 축에 표시\n",
    "       # result_image: YOLO 결과가 그려진 이미지 배열\n",
    "       # RGB 또는 BGR 형식의 3차원 numpy 배열을 자동으로 처리\n",
    "       \n",
    "       plt.title(\"Object Detection Result\")\n",
    "       # 그래프 제목 설정으로 사용자에게 이것이 객체 검출 결과임을 명시\n",
    "       \n",
    "       plt.axis('off')\n",
    "       # x, y축 눈금과 라벨을 제거해서 이미지만 깔끔하게 표시\n",
    "       # 좌표축이 있으면 이미지 감상에 방해가 되므로 제거\n",
    "       \n",
    "       plt.show()\n",
    "       # 실제로 화면에 그래프와 이미지를 표시\n",
    "       # Jupyter 노트북에서는 셀 출력으로 인라인 표시\n",
    "       \n",
    "       # 인식된 객체들의 상세 정보 출력\n",
    "       boxes = results[0].boxes\n",
    "       # .boxes: 검출된 모든 객체의 바운딩 박스 정보 모음\n",
    "       # 각 박스는 좌표, 클래스, 신뢰도 정보를 포함\n",
    "       # None일 수도 있음 (아무 객체도 검출되지 않은 경우)\n",
    "       \n",
    "       if boxes is not None:\n",
    "           # 객체가 하나라도 인식되었다면 상세 정보 출력\n",
    "           print(f\"✅ {len(boxes)} 개의 객체를 발견했습니다!\")\n",
    "           # ✅ 체크마크로 성공적인 검출을 표현\n",
    "           # len(boxes): 검출된 총 객체 개수\n",
    "           \n",
    "           # 각 객체의 정보를 하나씩 출력하는 반복문\n",
    "           for i, box in enumerate(boxes):\n",
    "               # enumerate(): 순서번호와 박스 정보를 동시에 가져옴\n",
    "               # i: 0부터 시작하는 객체 순서 번호\n",
    "               # box: 개별 객체의 검출 정보 (좌표, 클래스, 신뢰도)\n",
    "               \n",
    "               class_name = model.names[int(box.cls)]\n",
    "               # box.cls: 검출된 객체의 클래스 ID (숫자)\n",
    "               # int(): 텐서를 정수로 변환\n",
    "               # model.names[]: 클래스 ID를 실제 객체 이름으로 변환\n",
    "               # 예: 0 → 'person', 2 → 'car', 5 → 'bus'\n",
    "               \n",
    "               confidence = float(box.conf)\n",
    "               # box.conf: 해당 객체에 대한 모델의 신뢰도 (0~1)\n",
    "               # float(): 텐서를 실수로 변환\n",
    "               # 높을수록 모델이 더 확신하는 검출 결과\n",
    "               \n",
    "               print(f\"   {i+1}. {class_name}: {confidence:.2f} (신뢰도)\")\n",
    "               # 순서 번호(1부터), 객체 이름, 신뢰도를 한 줄로 출력\n",
    "               # {confidence:.2f}: 소수점 둘째자리까지 표시 (예: 0.85)\n",
    "               # 들여쓰기 3칸으로 하위 정보임을 시각적으로 표현\n",
    "               \n",
    "       else:\n",
    "           # 아무 객체도 인식되지 않은 경우의 처리\n",
    "           print(\"❌ 객체를 찾지 못했습니다.\")\n",
    "           # ❌ X 표시로 검출 실패를 명확히 표현\n",
    "           # 이미지 품질, 객체 크기, 신뢰도 임계값 등이 원인일 수 있음\n",
    "           \n",
    "   except Exception as e:\n",
    "       # Exception: 모든 종류의 예외를 포괄적으로 처리\n",
    "       # 네트워크 오류, 파일 형식 오류, 메모리 부족 등 다양한 문제 대응\n",
    "       print(f\"❌ 오류 발생: {e}\")\n",
    "       # f-string으로 구체적인 오류 메시지를 사용자에게 표시\n",
    "       # 디버깅과 문제 해결에 도움이 되는 정보 제공\n",
    "       \n",
    "       print(\"💡 인터넷 연결을 확인하거나 다른 이미지를 사용해보세요.\")\n",
    "       # 💡 전구 이모지로 해결 방법 제안임을 표현\n",
    "       # 일반적인 해결 방법을 친절하게 안내해서 사용자 편의성 향상\n",
    "\n",
    "# 첫 번째 샘플 이미지로 테스트 실행\n",
    "download_and_test_image(sample_urls[0], \"버스 이미지\")\n",
    "# 정의한 함수를 실제로 호출해서 첫 번째 샘플 이미지 테스트\n",
    "# sample_urls[0]: 버스 이미지 URL\n",
    "# \"버스 이미지\": 사용자에게 표시할 설명 텍스트\n",
    "# 복잡한 도시 장면에서 다중 객체 검출 성능 확인\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "# \\n으로 줄바꿈을 추가해서 다음 섹션과 시각적으로 분리\n",
    "# = 문자 50개로 이전보다 긴 구분선 생성\n",
    "# 샘플 이미지 테스트 섹션의 완료를 명확히 표시\n",
    "\n",
    "# 💡 이 코드에서 배울 수 있는 핵심 개념들:\n",
    "# 1. 인터넷에서 이미지를 다운로드하고 AI 모델에 입력하는 완전한 워크플로우\n",
    "# 2. YOLO 모델의 추론 과정과 결과 해석 방법 (바운딩 박스, 클래스, 신뢰도)\n",
    "# 3. 컴퓨터 비전 결과의 시각화 기법 (matplotlib + PIL 조합)\n",
    "# 4. 예외 처리를 통한 안정적인 네트워크 프로그래밍\n",
    "# 5. 함수 설계를 통한 코드 재사용성과 모듈화\n",
    "# 6. 실제 AI 모델 사용시 고려해야 할 매개변수들 (confidence, IoU threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ffc322",
   "metadata": {},
   "source": [
    "## 📹 5단계: 웹캠 환경 설정하기\n",
    "\n",
    "### 🎯 학습 목표\n",
    "정적인 이미지 테스트를 마쳤으니, 이제 실시간 영상 처리를 위한 웹캠 환경을 설정해보겠습니다. 이는 실시간 객체 인식의 핵심 준비 단계입니다.\n",
    "\n",
    "### 🔧 웹캠 설정의 중요성\n",
    "\n",
    "**하드웨어 호환성 확인** 🖥️\n",
    "- 운영체제마다 웹캠 접근 방식이 다릅니다\n",
    "- 드라이버와 권한 설정이 올바른지 확인해야 합니다\n",
    "- 다른 프로그램에서 웹캠을 사용 중이면 충돌이 발생할 수 있습니다\n",
    "\n",
    "**최적 성능을 위한 설정** ⚡\n",
    "- **해상도**: 1280x720 (HD 품질과 처리 속도의 균형)\n",
    "- **FPS**: 30프레임 (자연스러운 영상과 실시간 처리 가능)\n",
    "- **색상 공간**: 일반적으로 BGR 포맷 사용\n",
    "\n",
    "### 📐 해상도와 성능의 관계\n",
    "\n",
    "**해상도별 특징** 📊\n",
    "- **640x480**: 빠른 처리, 낮은 품질\n",
    "- **1280x720 (HD)**: 균형잡힌 선택 ⭐\n",
    "- **1920x1080 (FHD)**: 높은 품질, 느린 처리\n",
    "- **3840x2160 (4K)**: 최고 품질, 매우 느린 처리\n",
    "\n",
    "**우리가 선택한 1280x720의 장점** 🎯\n",
    "- GPU에서 실시간 처리 가능\n",
    "- 객체 인식에 충분한 해상도\n",
    "- 메모리 사용량과 속도의 적절한 균형\n",
    "\n",
    "### 🎬 FPS (Frames Per Second) 이해하기\n",
    "\n",
    "**FPS의 의미** ⏱️\n",
    "- 1초에 처리하는 프레임(이미지) 수\n",
    "- 높을수록 더 부드러운 영상\n",
    "- 실시간 처리를 위해서는 최소 15-20 FPS 필요\n",
    "\n",
    "**목표 성능** 🏆\n",
    "- **웹캠 입력**: 30 FPS\n",
    "- **AI 처리 후**: 25+ FPS (실시간 체감)\n",
    "- **GPU 가속 효과**: CPU 대비 10-50배 빠름\n",
    "\n",
    "### 🛠️ 웹캠 설정 과정\n",
    "\n",
    "**단계별 진행** 📋\n",
    "1. **연결 확인**: `cv2.VideoCapture(0)`로 웹캠 접근\n",
    "2. **해상도 설정**: 원하는 크기로 조정\n",
    "3. **FPS 설정**: 최적 프레임 속도 설정\n",
    "4. **테스트 캡처**: 실제 프레임이 잘 들어오는지 확인\n",
    "\n",
    "**일반적인 문제들** ⚠️\n",
    "- **웹캠 번호 오류**: 0번이 안 되면 1, 2, 3번 시도\n",
    "- **권한 문제**: 카메라 접근 권한 허용 필요\n",
    "- **다중 사용**: Zoom, Teams 등에서 웹캠 사용 중이면 충돌\n",
    "\n",
    "### 💡 테스트 캡처의 중요성\n",
    "\n",
    "**왜 테스트가 필요한가?** 🧪\n",
    "- 웹캠이 연결되어도 실제 영상이 안 나올 수 있습니다\n",
    "- 해상도나 FPS 설정이 실제로 적용되었는지 확인해야 합니다\n",
    "- 다음 단계 진행 전에 미리 문제를 발견할 수 있습니다\n",
    "\n",
    "**3초 미리보기** 👀\n",
    "- 웹캠 영상이 정상적으로 나오는지 빠르게 확인\n",
    "- 조명 상태와 화질을 미리 체크\n",
    "- 실시간 인식에 적합한 환경인지 판단\n",
    "\n",
    "### 🎓 실전에서의 웹캠 활용\n",
    "\n",
    "**다양한 응용 분야** 🌐\n",
    "- **화상 회의**: Zoom, Teams의 배경 블러 기능\n",
    "- **보안 시스템**: 출입 통제, 침입자 감지\n",
    "- **스마트 미러**: 옷 추천, 건강 체크\n",
    "- **게임**: 모션 컨트롤, AR 게임\n",
    "\n",
    "**성능 최적화 팁** 🚀\n",
    "- 조명이 밝고 균일한 환경에서 테스트\n",
    "- 배경이 복잡하지 않은 곳에서 시작\n",
    "- 카메라와 적정 거리(1-3m) 유지\n",
    "\n",
    "이제 웹캠을 설정하고 AI가 실제 세계를 \"볼\" 준비를 해보겠습니다! 📹✨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb12081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 5단계: 웹캠 환경 설정하기\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "다섯 번째 단계: 웹캠을 설정하고 연결해봅시다!\n",
    "실시간 객체 인식의 준비 단계입니다.\n",
    "\"\"\"\n",
    "# 독스트링으로 이 단계의 목적과 중요성을 설명함\n",
    "# 웹캠 설정: 실시간 영상에서 AI 객체 인식을 수행하기 위한 필수 전 단계\n",
    "# 하드웨어와 소프트웨어 연결의 안정성을 확보하는 중요한 과정\n",
    "\n",
    "# 웹캠 설정 단계 시작 알림\n",
    "print(\"📹 웹캠 설정하기\")\n",
    "# 📹 비디오 카메라 이모지로 웹캠 관련 작업임을 직관적으로 표현\n",
    "# 사용자에게 하드웨어 설정 단계임을 명확히 알림\n",
    "\n",
    "print(\"=\"*17)\n",
    "# = 문자 17개로 구분선을 만들어서 이전 섹션과 시각적으로 분리\n",
    "# 각 섹션마다 다른 길이의 구분선으로 구조적 계층 표현\n",
    "\n",
    "def setup_webcam():\n",
    "   \"\"\"웹캠 설정 및 테스트하는 함수\"\"\"\n",
    "   # 함수 정의: 웹캠 설정 과정을 독립적인 함수로 분리\n",
    "   # 코드 재사용성, 오류 격리, 구조화된 프로그래밍의 장점\n",
    "   # 반환값으로 설정된 웹캠 객체를 제공해서 다음 단계에서 활용 가능\n",
    "   \n",
    "   # 웹캠 연결 시도 (0번 장치 = 기본 웹캠)\n",
    "   cap = cv2.VideoCapture(0)\n",
    "   # cv2.VideoCapture(): OpenCV의 비디오 입력 클래스\n",
    "   # 매개변수 0: 시스템의 첫 번째 카메라 장치\n",
    "   #   0 = 기본 웹캠 (노트북 내장 카메라 또는 첫 번째 USB 카메라)\n",
    "   #   1 = 두 번째 카메라 (외장 USB 카메라 등)\n",
    "   #   2, 3, ... = 추가 카메라 장치들\n",
    "   # 파일 경로를 넣으면 비디오 파일도 재생 가능\n",
    "   \n",
    "   # 웹캠이 제대로 열렸는지 확인\n",
    "   if not cap.isOpened():\n",
    "       # cap.isOpened(): 카메라 장치가 성공적으로 열렸는지 확인하는 메서드\n",
    "       # True: 연결 성공, False: 연결 실패\n",
    "       # not으로 부정해서 실패한 경우를 처리\n",
    "       \n",
    "       # 연결 실패의 주요 원인들:\n",
    "       # 1. 카메라 하드웨어가 없음\n",
    "       # 2. 다른 프로그램에서 카메라 사용 중 (리소스 점유)\n",
    "       # 3. 카메라 드라이버 문제\n",
    "       # 4. 권한 문제 (개인정보 보호 설정)\n",
    "       # 5. 하드웨어 오류나 연결 불량\n",
    "       \n",
    "       print(\"❌ 웹캠을 열 수 없습니다!\")\n",
    "       # ❌ X 표시로 실패를 명확히 표현\n",
    "       \n",
    "       print(\"💡 다른 프로그램에서 웹캠을 사용 중인지 확인해보세요.\")\n",
    "       # 💡 전구 이모지로 해결 방법 제안\n",
    "       # 가장 흔한 원인인 리소스 충돌 문제를 우선적으로 안내\n",
    "       # Zoom, Microsoft Teams, 스카이프, 카카오톡 등이 웹캠을 점유할 수 있음\n",
    "       \n",
    "       return None\n",
    "       # None 반환으로 설정 실패를 명시적으로 표시\n",
    "       # 호출하는 코드에서 None 체크로 다음 단계 진행 여부 결정 가능\n",
    "       \n",
    "   # 웹캠 해상도와 FPS 설정\n",
    "   cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)\n",
    "   # cv2.CAP_PROP_FRAME_WIDTH: 프레임 너비를 설정하는 상수\n",
    "   # 1280픽셀: HD(High Definition) 해상도의 너비\n",
    "   # 높은 해상도일수록 더 선명하지만 처리 부하 증가\n",
    "   \n",
    "   cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)\n",
    "   # cv2.CAP_PROP_FRAME_HEIGHT: 프레임 높이를 설정하는 상수\n",
    "   # 720픽셀: HD 해상도의 높이\n",
    "   # 1280x720 = 720p HD 해상도 (16:9 비율)\n",
    "   # AI 객체 인식에 적합한 품질과 성능의 균형점\n",
    "   \n",
    "   cap.set(cv2.CAP_PROP_FPS, 30)\n",
    "   # cv2.CAP_PROP_FPS: 초당 프레임 수를 설정하는 상수\n",
    "   # 30fps: 부드러운 영상을 위한 표준 프레임레이트\n",
    "   # 더 높은 FPS는 더 부드럽지만 처리 부하와 대역폭 증가\n",
    "   # 실시간 AI 처리에서는 15-30fps가 적절한 범위\n",
    "   \n",
    "   # 실제로 설정된 값들을 다시 읽어와서 확인\n",
    "   # (하드웨어가 요청한 설정을 지원하지 않을 수 있음)\n",
    "   width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "   # cap.get(): 현재 설정된 속성값을 가져오는 메서드\n",
    "   # 요청한 값과 실제 설정된 값이 다를 수 있음\n",
    "   # 웹캠 하드웨어 제한이나 드라이버 제약으로 인한 차이\n",
    "   \n",
    "   height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "   # 실제 설정된 높이값 확인\n",
    "   # int() 변환: get() 메서드는 float을 반환하므로 정수로 변환\n",
    "   \n",
    "   fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "   # 실제 설정된 FPS값 확인\n",
    "   # 일부 웹캠은 30fps를 지원하지 않아 15fps나 24fps로 설정될 수 있음\n",
    "   \n",
    "   # 웹캠 연결 성공과 설정값 출력\n",
    "   print(f\"✅ 웹캠 연결 성공!\")\n",
    "   # ✅ 체크마크로 성공적인 연결을 시각적으로 표현\n",
    "   \n",
    "   print(f\"📐 해상도: {width} x {height}\")\n",
    "   # 📐 자 이모지로 크기/해상도 정보임을 표현\n",
    "   # f-string으로 실제 설정된 해상도를 동적으로 표시\n",
    "   # 사용자가 실제 작업 환경을 정확히 파악할 수 있게 함\n",
    "   \n",
    "   print(f\"🎬 FPS: {fps}\")\n",
    "   # 🎬 영화 카메라 이모지로 프레임레이트 정보임을 표현\n",
    "   # 실제 FPS를 확인해서 성능 예측과 최적화 기준 제공\n",
    "   \n",
    "   # 테스트용으로 한 프레임 캡처해보기\n",
    "   ret, frame = cap.read()\n",
    "   # cap.read(): 웹캠에서 현재 프레임을 읽어오는 메서드\n",
    "   # 반환값:\n",
    "   #   ret: 읽기 성공 여부 (True/False)\n",
    "   #   frame: 실제 이미지 데이터 (numpy 배열)\n",
    "   # 이 테스트로 웹캠이 실제로 영상을 제공하는지 확인\n",
    "   \n",
    "   if ret:\n",
    "       # 프레임을 성공적으로 읽어왔다면\n",
    "       print(\"📸 테스트 프레임 캡처 성공!\")\n",
    "       # 📸 카메라 이모지로 촬영 성공을 표현\n",
    "       \n",
    "       # 캡처한 프레임을 잠깐 화면에 표시해서 웹캠이 정상 작동하는지 확인\n",
    "       cv2.imshow('Webcam Test - Auto Close in 3 sec', frame)\n",
    "       # cv2.imshow(): OpenCV 창에 이미지를 표시\n",
    "       # 창 제목에 자동 종료 안내를 포함해서 사용자 혼란 방지\n",
    "       # frame: 방금 캡처한 실제 웹캠 영상\n",
    "       \n",
    "       cv2.waitKey(3000)\n",
    "       # 3000ms = 3초 동안 키 입력 대기\n",
    "       # 사용자가 웹캠 영상이 정상인지 확인할 충분한 시간 제공\n",
    "       # 키를 누르면 즉시 진행, 아니면 3초 후 자동 진행\n",
    "       \n",
    "       cv2.destroyAllWindows()\n",
    "       # 열린 모든 OpenCV 창을 닫고 메모리 정리\n",
    "       # 테스트 창이 남아있으면 다음 단계에서 방해가 될 수 있음\n",
    "       # 깔끔한 환경으로 다음 단계 준비\n",
    "       \n",
    "   return cap\n",
    "   # 설정 완료된 웹캠 객체를 반환\n",
    "   # 이 객체를 통해 계속해서 프레임을 읽어와서 AI 처리 수행\n",
    "   # None이 아닌 유효한 객체 반환은 성공적인 설정의 증거\n",
    "\n",
    "# 웹캠 설정 함수 실행\n",
    "webcam = setup_webcam()\n",
    "# 정의한 함수를 실제로 호출해서 웹캠 설정 수행\n",
    "# 반환된 웹캠 객체를 webcam 변수에 저장\n",
    "# 이 변수는 다음 단계들에서 실시간 영상 처리에 계속 사용됨\n",
    "# 전역 변수로 저장해서 다른 함수들에서도 접근 가능\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "# \\n으로 줄바꿈을 추가해서 다음 섹션과 시각적으로 분리\n",
    "# = 문자 50개로 이전보다 긴 구분선 생성\n",
    "# 웹캠 설정 단계의 완료를 명확히 표시\n",
    "\n",
    "# 💡 이 코드에서 배우는 핵심 개념들:\n",
    "# 1. VideoCapture 객체: OpenCV에서 비디오 입력을 처리하는 핵심 클래스\n",
    "#    - 웹캠, 비디오 파일, IP 카메라 등 다양한 소스 지원\n",
    "#    - 프레임 단위로 영상 데이터 접근 가능\n",
    "#    - 해상도, FPS, 코덱 등 다양한 속성 제어 가능\n",
    "#\n",
    "# 2. 해상도 설정의 중요성: AI 처리 속도와 화질 사이의 균형점\n",
    "#    - 높은 해상도: 더 정확한 객체 인식, 하지만 느린 처리 속도\n",
    "#    - 낮은 해상도: 빠른 처리 속도, 하지만 작은 객체 놓칠 가능성\n",
    "#    - 1280x720 (HD): 실시간 AI 처리에 적합한 절충점\n",
    "#\n",
    "# 3. 프레임 캡처 메커니즘: 연속된 이미지들을 하나씩 가져오는 과정\n",
    "#    - cap.read()로 실시간 영상 스트림에서 정지 이미지 추출\n",
    "#    - ret 값으로 성공/실패 확인하는 안전한 프로그래밍 패턴\n",
    "#    - numpy 배열 형태로 이미지 데이터 제공\n",
    "#\n",
    "# 4. 하드웨어 호환성과 오류 처리:\n",
    "#    - 요청한 설정과 실제 설정이 다를 수 있음을 인식\n",
    "#    - 리소스 충돌, 권한 문제 등 다양한 실패 원인 대비\n",
    "#    - 사용자 친화적인 오류 메시지와 해결 방법 제공\n",
    "#\n",
    "# 🎯 성공 조건: webcam 변수가 None이 아닌 유효한 객체여야 다음 단계 진행 가능!\n",
    "# 실패시에는 웹캠 관련 문제를 해결한 후 이 셀을 다시 실행해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946eaf26",
   "metadata": {},
   "source": [
    "## 🎥 6단계: 실시간 객체 인식 - 기본 버전\n",
    "\n",
    "### 🎯 학습 목표\n",
    "드디어 본격적인 실시간 객체 인식을 시작해보겠습니다! 웹캠으로 들어오는 영상을 실시간으로 분석하여 객체를 찾아내는 놀라운 경험을 해보세요.\n",
    "\n",
    "### 🚀 실시간 처리의 의미\n",
    "\n",
    "**진짜 \"실시간\"이란?** ⏱️\n",
    "- 웹캠에서 들어오는 영상을 지연 없이 바로 처리\n",
    "- 초당 20-30프레임을 연속으로 분석\n",
    "- 사람의 눈으로 봤을 때 자연스럽고 부드러운 영상\n",
    "\n",
    "**기존 방식과의 차이** 📊\n",
    "- **정적 이미지**: 한 장씩 처리 → 결과 확인 → 다음 이미지\n",
    "- **실시간 영상**: 연속적인 프레임 → 즉석 분석 → 즉시 결과 표시\n",
    "\n",
    "### ⌨️ 키보드 조작법\n",
    "\n",
    "**기본 조작** 🎮\n",
    "- **'q' 키**: 프로그램 종료 (Quit)\n",
    "- **'s' 키**: 현재 화면 스크린샷 저장 (Screenshot)\n",
    "- **'f' 키**: FPS 정보 표시/숨기기 토글 (FPS toggle)\n",
    "\n",
    "**ESC나 X 버튼은?** ❌\n",
    "- 윈도우 창의 X 버튼으로 닫으면 프로그램이 완전히 종료되지 않을 수 있음\n",
    "- 반드시 'q' 키를 사용해서 정상 종료하세요!\n",
    "\n",
    "### 📊 성능 모니터링\n",
    "\n",
    "**FPS (Frames Per Second)** 🎬\n",
    "- 실시간 처리 성능의 핵심 지표\n",
    "- **30+ FPS**: 매우 부드러운 실시간 처리\n",
    "- **20-30 FPS**: 실용적인 실시간 처리\n",
    "- **10-20 FPS**: 약간 끊김 있지만 사용 가능\n",
    "- **10 FPS 미만**: 실시간이라 하기 어려움\n",
    "\n",
    "**Device 정보** 🖥️\n",
    "- `cuda:0`: 첫 번째 GPU 사용 중\n",
    "- `cpu`: CPU로 처리 중\n",
    "- GPU 사용시 10-50배 빠른 성능!\n",
    "\n",
    "### 🎨 화면 구성 요소\n",
    "\n",
    "**객체 인식 결과** 🎯\n",
    "- **바운딩 박스**: 인식된 객체를 감싸는 색상 테두리\n",
    "- **라벨**: 객체 이름 (person, car, phone 등)\n",
    "- **신뢰도**: 0.0~1.0 사이의 확신 정도\n",
    "\n",
    "**성능 정보 표시** 📈\n",
    "- 좌측 상단에 실시간 통계 정보\n",
    "- 검은 배경에 녹색 글씨로 가독성 확보\n",
    "- FPS 토글로 필요시 숨기기 가능\n",
    "\n",
    "### 💡 첫 실행시 주의사항\n",
    "\n",
    "**처음 몇 초는 느릴 수 있음** ⏳\n",
    "- GPU 워밍업 시간 필요\n",
    "- 모델 로딩과 메모리 할당 과정\n",
    "- 2-3초 후 정상 속도로 안정화\n",
    "\n",
    "**조명과 환경** 💡\n",
    "- 밝고 균일한 조명에서 최적 성능\n",
    "- 너무 어둡거나 역광이면 인식률 저하\n",
    "- 배경이 복잡하면 처리 시간 증가\n",
    "\n",
    "### 🔬 관찰해볼 점들\n",
    "\n",
    "**인식 정확도** ✅\n",
    "- 명확한 객체들이 정확히 인식되는지 확인\n",
    "- 거리별, 각도별 인식 성능 차이 관찰\n",
    "- 빠른 움직임에 대한 추적 능력 테스트\n",
    "\n",
    "**성능 변화** 📉\n",
    "- 화면에 객체가 많을 때 vs 적을 때\n",
    "- 사람이 많을 때 vs 혼자 있을 때\n",
    "- 복잡한 배경 vs 단순한 배경\n",
    "\n",
    "### 🎓 실전 응용 아이디어\n",
    "\n",
    "**보안 시스템** 🔒\n",
    "- 특정 영역에 사람이 들어오면 알림\n",
    "- 허가되지 않은 객체 감지\n",
    "\n",
    "**스마트 홈** 🏠\n",
    "- 거실에 있는 사람 수 자동 카운팅\n",
    "- 반려동물 활동 모니터링\n",
    "\n",
    "**상업적 활용** 💼\n",
    "- 매장 내 고객 동선 분석\n",
    "- 제품 진열 효과 측정\n",
    "\n",
    "### ⚠️ 문제 해결 가이드\n",
    "\n",
    "**화면이 느려질 때** 🐌\n",
    "- 다른 프로그램들 종료\n",
    "- 웹캠 해상도 낮춤 (640x480)\n",
    "- 신뢰도 임계값 높임 (0.7 이상)\n",
    "\n",
    "**인식이 안 될 때** 🤔\n",
    "- 조명 개선\n",
    "- 카메라와 적절한 거리 유지\n",
    "- 객체가 카메라 시야에 완전히 들어오도록\n",
    "\n",
    "이제 AI의 눈으로 세상을 실시간으로 바라보는 마법 같은 경험을 시작해보세요! 🔮✨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75be4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6단계: 실시간 객체 인식  \n",
    "# =============================================================================\n",
    "\n",
    "\"\"\"\n",
    "여섯 번째 단계: 드디어 실시간 객체 인식을 해봅시다!\n",
    "'q' 키를 누르면 종료됩니다.\n",
    "\"\"\"\n",
    "# 독스트링으로 이 단계의 흥미진진함과 핵심 조작법을 강조함\n",
    "# 실시간 객체 인식: 이전 단계들의 모든 준비가 결합되어 완성되는 최종 목표\n",
    "# 정적 이미지가 아닌 연속적인 영상에서 실시간으로 객체를 찾아내는 AI 시스템\n",
    "\n",
    "# 실시간 처리에 필요한 추가 라이브러리들을 가져옴\n",
    "import time\n",
    "# time 모듈: 시간 측정과 관련된 기능 제공함\n",
    "# FPS 계산, 성능 분석, 타임스탬프 생성 등에 필수적임\n",
    "# time.time(): 현재 시간을 초 단위로 반환함 (성능 측정의 기준점)\n",
    "\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "# PIL (Python Image Library): 이미지 처리 라이브러리\n",
    "# ImageDraw: 이미지에 도형과 텍스트를 그리는 기능\n",
    "# ImageFont: 다양한 폰트 처리함 (한글 폰트 문제 해결용)\n",
    "# OpenCV의 한글 폰트 제한을 PIL로 보완하는 전략\n",
    "\n",
    "import numpy as np\n",
    "# NumPy: 수치 연산과 배열 처리 라이브러리\n",
    "# 이미지 데이터는 본질적으로 숫자 배열이므로 NumPy로 효율적 처리함\n",
    "# OpenCV와 PIL 간의 데이터 형식 변환에도 활용됨\n",
    "\n",
    "# 한글 폰트 설정 함수 (현재는 사용하지 않지만 나중에 필요할 때를 위해 준비함)\n",
    "def put_korean_text(img, text, position, font_size=20, color=(0, 255, 0)):\n",
    "    \"\"\"\n",
    "    OpenCV 이미지에 한글 텍스트를 표시하는 함수\n",
    "    PIL을 사용하여 한글 폰트 문제를 해결함\n",
    "    \"\"\"\n",
    "    # 이 함수는 OpenCV의 한글 폰트 지원 한계를 극복하기 위한 해결책\n",
    "    # OpenCV의 cv2.putText()는 한글을 제대로 표시하지 못하는 문제가 있음\n",
    "    # PIL을 중간 매개체로 사용해서 한글 텍스트 렌더링 후 OpenCV로 변환함\n",
    "    \n",
    "    # BGR 색상을 RGB로 변환함 (OpenCV와 PIL의 색상 순서가 다름)\n",
    "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    # OpenCV는 BGR(파랑-초록-빨강) 순서 사용함\n",
    "    # PIL은 RGB(빨강-초록-파랑) 순서 사용함\n",
    "    # 색상 변환 없이 사용하면 색깔이 이상하게 나타남\n",
    "    \n",
    "    img_pil = Image.fromarray(img_rgb)\n",
    "    # NumPy 배열을 PIL Image 객체로 변환함\n",
    "    # fromarray(): 배열 데이터를 이미지로 해석하는 PIL 메서드\n",
    "    \n",
    "    draw = ImageDraw.Draw(img_pil)\n",
    "    # PIL에서 그리기 작업을 수행할 수 있는 Draw 객체 생성함\n",
    "    # 이 객체를 통해 텍스트, 도형 등을 이미지에 그릴 수 있음\n",
    "    \n",
    "    # 시스템에 설치된 한글 폰트를 찾기 시도함\n",
    "    try:\n",
    "        font = ImageFont.truetype(\"malgun.ttf\", font_size)\n",
    "        # Windows 시스템의 맑은고딕 폰트 사용을 시도함\n",
    "        # truetype(): TrueType 폰트 파일을 로드하는 메서드\n",
    "    except:\n",
    "        try:\n",
    "            font = ImageFont.truetype(\"NanumGothic.ttf\", font_size)\n",
    "            # Linux/Mac 시스템의 나눔고딕 폰트 사용을 시도함\n",
    "            # 다양한 운영체제 환경을 고려한 폰트 대안\n",
    "        except:\n",
    "            font = ImageFont.load_default()\n",
    "            # 모든 폰트 로드가 실패하면 기본 폰트를 사용함\n",
    "            # 기본 폰트는 한글 지원이 제한적이지만 오류는 방지함\n",
    "    \n",
    "    # PIL로 한글 텍스트를 그림\n",
    "    draw.text(position, text, font=font, fill=color)\n",
    "    # position: 텍스트가 그려질 좌표 (x, y)\n",
    "    # text: 실제 표시할 한글 문자열\n",
    "    # font: 위에서 로드한 폰트 객체\n",
    "    # fill: 텍스트 색상 (RGB 형식)\n",
    "    \n",
    "    # RGB를 다시 BGR로 변환해서 OpenCV에서 사용 가능하게 만듦\n",
    "    img_bgr = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)\n",
    "    # np.array(): PIL 이미지를 다시 NumPy 배열로 변환함\n",
    "    # COLOR_RGB2BGR: PIL의 RGB를 OpenCV의 BGR로 재변환함\n",
    "    # 최종적으로 OpenCV에서 사용 가능한 형태로 반환함\n",
    "    \n",
    "    return img_bgr\n",
    "\n",
    "# 실시간 인식 시작을 알림\n",
    "print(\"🎥 실시간 객체 인식 시작!\")\n",
    "# 🎥 영화 카메라 이모지로 동영상 처리의 시작을 표현함\n",
    "# 드디어 모든 준비가 끝나고 본격적인 AI 인식이 시작됨을 알림\n",
    "\n",
    "print(\"=\"*28)\n",
    "# = 문자 28개로 구분선을 만들어서 이전 섹션과 분리함\n",
    "\n",
    "print(\"💡 조작법:\")\n",
    "print(\"   - 'q' 키: 종료\")\n",
    "print(\"   - 's' 키: 스크린샷 저장\")\n",
    "print(\"   - 'f' 키: FPS 정보 토글\")\n",
    "print()\n",
    "# 💡 전구 이모지로 도움말임을 표시함\n",
    "# 사용자가 프로그램 실행 중에 할 수 있는 모든 조작 방법을 미리 안내함\n",
    "# q: quit (종료), s: screenshot (화면 저장), f: fps (성능 정보)\n",
    "# 직관적인 키 배치로 사용자 편의성을 극대화함\n",
    "\n",
    "def real_time_detection_basic():\n",
    "    \"\"\"기본 실시간 객체 인식 함수\"\"\"\n",
    "    # 실시간 객체 인식의 모든 기능을 담은 핵심 함수\n",
    "    # 웹캠 입력부터 AI 처리, 결과 표시까지의 전체 파이프라인을 구현함\n",
    "    \n",
    "    # 웹캠이 제대로 설정되었는지 먼저 확인함\n",
    "    if webcam is None:\n",
    "        # 이전 단계에서 웹캠 설정이 실패했다면 webcam 변수가 None임\n",
    "        print(\"❌ 웹캠이 준비되지 않았습니다!\")\n",
    "        # 웹캠 없이는 실시간 인식이 불가능함을 명확히 알림\n",
    "        return\n",
    "        # 함수를 즉시 종료해서 오류 상황을 방지함\n",
    "    \n",
    "    # 성능 측정을 위한 변수들을 초기화함\n",
    "    frame_count = 0\n",
    "    # 처리한 총 프레임 수를 카운트하는 변수\n",
    "    # FPS 계산과 성능 분석에 핵심적인 지표임\n",
    "    \n",
    "    start_time = time.time()\n",
    "    # 프로그램 시작 시점의 타임스탬프를 기록함\n",
    "    # 경과 시간 계산의 기준점으로 사용됨\n",
    "    \n",
    "    show_fps = True\n",
    "    # FPS 정보 표시 여부를 제어하는 플래그\n",
    "    # 기본값 True: 성능 정보를 화면에 표시함\n",
    "    # 'f' 키로 토글 가능해서 사용자가 선택적으로 조절함\n",
    "    \n",
    "    print(\"🚀 실시간 인식 시작! (q키로 종료)\")\n",
    "    # 🚀 로켓 이모지로 빠른 실시간 처리의 시작을 표현함\n",
    "    # 종료 방법을 다시 한 번 강조해서 사용자 편의성을 제공함\n",
    "    \n",
    "    try:\n",
    "        # try 블록: 실시간 처리 중 발생할 수 있는 모든 오류에 대비함\n",
    "        # 웹캠 오류, 메모리 부족, 키보드 인터럽트 등에 안전하게 대응함\n",
    "        \n",
    "        while True:\n",
    "            # 무한 루프: 실시간 영상 처리의 핵심 구조\n",
    "            # 연속적인 프레임을 끊임없이 처리해서 실시간성을 확보함\n",
    "            # break 문을 만나기 전까지 계속 실행됨\n",
    "            \n",
    "            # 웹캠에서 한 프레임(이미지)을 읽어옴\n",
    "            ret, frame = webcam.read()\n",
    "            # webcam.read(): 웹캠에서 현재 프레임을 가져오는 메서드\n",
    "            # ret: 읽기 성공 여부 (True/False)\n",
    "            # frame: 실제 이미지 데이터 (numpy 배열)\n",
    "            # 실시간 처리에서는 이 과정이 초당 수십 번 반복됨\n",
    "            \n",
    "            if not ret:\n",
    "                # 프레임 읽기에 실패한 경우를 처리함\n",
    "                # 웹캠 연결 끊김, 하드웨어 오류 등이 원인임\n",
    "                print(\"❌ 프레임을 읽을 수 없습니다!\")\n",
    "                break\n",
    "                # while 루프를 종료해서 프로그램을 안전하게 마무리함\n",
    "            \n",
    "            # YOLO 모델로 객체 인식을 수행함 (실시간 처리의 핵심!)\n",
    "            results = model(frame, device=device, conf=0.5, verbose=False)\n",
    "            # model(): 훈련된 YOLO 모델에 현재 프레임을 입력해서 객체를 검출함\n",
    "            # 매개변수 세부 설명:\n",
    "            #   frame: 웹캠에서 방금 읽어온 이미지\n",
    "            #   device: GPU 또는 CPU (이전 단계에서 설정된 값)\n",
    "            #   conf=0.5: 신뢰도 임계값 50% (50% 이상 확신하는 객체만 검출함)\n",
    "            #   verbose=False: 상세 로그 출력을 안 함 (실시간 처리 속도 향상)\n",
    "            \n",
    "            # 인식 결과를 원본 프레임에 그려서 시각화함\n",
    "            annotated_frame = results[0].plot()\n",
    "            # results[0]: 첫 번째(유일한) 이미지의 검출 결과\n",
    "            # .plot(): 원본 이미지에 바운딩 박스, 라벨, 신뢰도를 그린 새 이미지를 생성함\n",
    "            # 각 객체마다 다른 색상의 사각형과 텍스트가 자동으로 추가됨\n",
    "            \n",
    "            # FPS를 계산함 (성능 모니터링용)\n",
    "            frame_count += 1\n",
    "            # 성공적으로 처리된 프레임 수를 1 증가시킴\n",
    "            # 이 값이 FPS 계산의 분자가 됨\n",
    "            \n",
    "            elapsed_time = time.time() - start_time\n",
    "            # 프로그램 시작부터 현재까지의 경과 시간을 계산함\n",
    "            # time.time() - start_time: 현재 시간에서 시작 시간을 뺀 차이\n",
    "            \n",
    "            fps = frame_count / elapsed_time if elapsed_time > 0 else 0\n",
    "            # FPS = 총 프레임 수 ÷ 경과 시간\n",
    "            # if elapsed_time > 0: 0으로 나누는 오류를 방지함\n",
    "            # 실시간 성능의 핵심 지표 (높을수록 더 부드러운 영상)\n",
    "            \n",
    "            # FPS 정보가 켜져있다면 화면에 표시함\n",
    "            if show_fps:\n",
    "                # show_fps 플래그가 True일 때만 성능 정보를 오버레이함\n",
    "                \n",
    "                # 표시할 정보들을 영어로 작성함 (한글 폰트 문제 해결)\n",
    "                info_texts = [\n",
    "                    f'FPS: {fps:.1f}',\n",
    "                    # 현재 FPS를 소수점 첫째 자리까지 표시함\n",
    "                    # 실시간 성능 모니터링의 핵심 지표\n",
    "                    \n",
    "                    f'Device: {device}',\n",
    "                    # 현재 사용 중인 처리 장치를 표시함 (cuda:0 또는 cpu)\n",
    "                    # GPU 사용 여부를 실시간으로 확인할 수 있음\n",
    "                    \n",
    "                    f'Frame: {frame_count}',\n",
    "                    # 프로그램 시작부터 처리한 총 프레임 수\n",
    "                    # 얼마나 많은 데이터를 처리했는지 추적함\n",
    "                    \n",
    "                    f'Conf: 0.5'\n",
    "                    # 현재 설정된 신뢰도 임계값을 표시함\n",
    "                    # 검출 민감도 설정을 사용자가 확인할 수 있음\n",
    "                ]\n",
    "                \n",
    "                # 정보 표시를 위한 배경 박스를 그림 (가독성 향상)\n",
    "                cv2.rectangle(annotated_frame, (5, 5), (300, 120), (0, 0, 0), -1)\n",
    "                # 검은색 실심 사각형 배경 (텍스트 가독성 향상)\n",
    "                # (5, 5): 시작점, (300, 120): 끝점\n",
    "                # (0, 0, 0): 검은색, -1: 내부를 채움\n",
    "                \n",
    "                cv2.rectangle(annotated_frame, (5, 5), (300, 120), (0, 255, 0), 2)\n",
    "                # 녹색 테두리로 정보 영역을 명확히 구분함\n",
    "                # (0, 255, 0): 녹색, 2: 테두리 두께\n",
    "                \n",
    "                # 각 정보 텍스트를 한 줄씩 표시함\n",
    "                for i, text in enumerate(info_texts):\n",
    "                    # enumerate(): 인덱스와 텍스트를 동시에 가져옴\n",
    "                    cv2.putText(annotated_frame, text, (10, 25 + i * 22), \n",
    "                               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "                    # 좌표 계산: (10, 25 + i * 22)\n",
    "                    #   x=10: 왼쪽에서 10픽셀 떨어진 위치\n",
    "                    #   y=25 + i * 22: 첫 줄은 25픽셀, 이후 22픽셀씩 아래로\n",
    "                    # 0.6: 적당한 폰트 크기, (0, 255, 0): 녹색, 2: 글자 두께\n",
    "            \n",
    "            # 완성된 프레임을 화면에 표시함\n",
    "            cv2.imshow('YOLO Real-time Detection', annotated_frame)\n",
    "            # cv2.imshow(): OpenCV 창에 최종 처리된 영상을 출력함\n",
    "            # 창 제목: 'YOLO Real-time Detection' (명확한 프로그램 식별)\n",
    "            # annotated_frame: AI 검출 결과와 성능 정보가 모두 포함된 완성 이미지\n",
    "            \n",
    "            # 키보드 입력을 확인함 (1ms 대기, 너무 오래 기다리면 실시간성이 떨어짐)\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            # cv2.waitKey(1): 1밀리초 동안만 키 입력을 대기함\n",
    "            # 1ms: 실시간성을 해치지 않는 최소한의 대기 시간\n",
    "            # & 0xFF: 키 코드를 8비트로 제한함 (특수키 처리 문제 해결)\n",
    "            \n",
    "            if key == ord('q'):\n",
    "                # ord('q'): 문자 'q'의 ASCII 코드 값 (113)\n",
    "                # 사용자가 'q' 키를 눌렀는지 확인함\n",
    "                print(\"👋 종료합니다!\")\n",
    "                # 👋 손흔들기 이모지로 친근한 작별 인사\n",
    "                break\n",
    "                # while 루프를 종료해서 프로그램을 정상 종료함\n",
    "                \n",
    "            elif key == ord('s'):\n",
    "                # 's' 키를 눌렀다면 (Screenshot 기능)\n",
    "                filename = f'screenshot_{int(time.time())}.jpg'\n",
    "                # 파일명 생성: 'screenshot_' + 타임스탬프 + '.jpg'\n",
    "                # int(time.time()): 현재 시간을 정수로 변환해서 고유성을 보장함\n",
    "                # 같은 시간에 여러 번 저장해도 파일명 충돌을 방지함\n",
    "                \n",
    "                cv2.imwrite(filename, annotated_frame)\n",
    "                # cv2.imwrite(): 현재 화면을 이미지 파일로 저장함\n",
    "                # annotated_frame: AI 결과와 성능 정보가 포함된 완성 이미지\n",
    "                # 사용자가 보고 있는 그대로의 화면이 저장됨\n",
    "                \n",
    "                print(f\"📸 스크린샷 저장: {filename}\")\n",
    "                # 📸 카메라 이모지로 촬영/저장 완료를 표현함\n",
    "                # 저장된 파일명을 표시해서 사용자가 찾기 쉽게 함\n",
    "                \n",
    "            elif key == ord('f'):\n",
    "                # 'f' 키를 눌렀다면 (FPS toggle 기능)\n",
    "                show_fps = not show_fps\n",
    "                # 불린 값을 반전함: True → False, False → True\n",
    "                # 성능 정보 표시를 사용자가 선택적으로 켜고 끌 수 있음\n",
    "                \n",
    "                print(f\"📊 FPS 표시: {'ON' if show_fps else 'OFF'}\")\n",
    "                # 📊 차트 이모지로 통계 정보임을 표현함\n",
    "                # 삼항 연산자로 현재 상태를 명확히 표시함\n",
    "    \n",
    "    except KeyboardInterrupt:\n",
    "        # KeyboardInterrupt: Ctrl+C를 눌러서 강제 종료한 경우\n",
    "        print(\"\\n⚠️  사용자에 의해 중단됨\")\n",
    "        # \\n으로 줄바꿈해서 깔끔한 출력\n",
    "        # ⚠️ 경고 이모지로 비정상 종료가 아닌 의도적 중단임을 표시함\n",
    "    \n",
    "    finally:\n",
    "        # finally 블록: try가 어떻게 끝나든 반드시 실행되는 정리 코드\n",
    "        # 정상 종료든 오류든 리소스 정리는 필수임\n",
    "        \n",
    "        # OpenCV 윈도우를 정리함\n",
    "        cv2.destroyAllWindows()\n",
    "        # 열린 모든 OpenCV 창을 닫고 메모리에서 제거함\n",
    "        # GUI 자원을 해제해서 시스템 안정성을 확보함\n",
    "        \n",
    "        # 최종 성능 통계를 출력함\n",
    "        total_time = time.time() - start_time\n",
    "        # 프로그램 시작부터 종료까지의 총 실행 시간\n",
    "        \n",
    "        avg_fps = frame_count / total_time if total_time > 0 else 0\n",
    "        # 전체 세션의 평균 FPS를 계산함\n",
    "        # 순간 FPS와 다른 전체적인 성능 지표\n",
    "        \n",
    "        print(f\"\\n📊 실행 결과:\")\n",
    "        print(f\"   총 프레임: {frame_count}\")\n",
    "        print(f\"   실행 시간: {total_time:.2f}초\")\n",
    "        print(f\"   평균 FPS: {avg_fps:.2f}\")\n",
    "        # 📊 차트 이모지로 통계 정보임을 표현함\n",
    "        # 들여쓰기로 하위 정보임을 시각적으로 구분함\n",
    "        # 소수점 둘째자리까지 표시해서 정밀한 성능 분석을 제공함\n",
    "\n",
    "# 실시간 인식 함수를 실행함\n",
    "real_time_detection_basic()\n",
    "# 위에서 정의한 함수를 실제로 호출해서 실시간 객체 인식을 시작함\n",
    "# 이 한 줄로 모든 준비가 완성되고 AI 비전 시스템이 작동을 시작함\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "# \\n으로 줄바꿈을 추가해서 실행 결과와 다음 섹션을 분리함\n",
    "# = 문자 50개로 긴 구분선을 생성해서 완료를 표시함\n",
    "\n",
    "# 💡 이 코드의 핵심 개념들과 학습 포인트:\n",
    "# 1. 무한 루프: 실시간 처리의 기본 구조와 연속성 보장 방법\n",
    "# 2. FPS 계산: 성능 측정의 표준 방법과 최적화 기준 이해\n",
    "# 3. 키보드 이벤트: 실시간 사용자 상호작용 처리 방법\n",
    "# 4. 예외 처리: 안전한 프로그램 종료와 오류 상황 대응 방법\n",
    "# 5. 메모리 정리: 시스템 자원 관리와 안정성 확보 방법\n",
    "# 6. 성능 최적화: 실시간성을 위한 처리 시간 최소화 기법\n",
    "# 7. 사용자 경험: 직관적인 조작법과 친근한 피드백 제공 방법\n",
    "#\n",
    "# 🚀 축하합니다! 이제 여러분은 실시간 AI 비전 시스템을 만들 수 있습니다!\n",
    "# 이 기술을 응용하면 스마트 보안 시스템, 자동차 자율주행, 로봇 비전 등 다양한 분야에 활용할 수 있습니다!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c479f12",
   "metadata": {},
   "source": [
    "## ⚖️ 7단계: 다양한 YOLO 모델 성능 비교\n",
    "\n",
    "### 🎯 학습 목표\n",
    "지금까지 하나의 YOLO 모델만 사용했는데, 이제 여러 크기의 모델을 비교해보겠습니다. 각 모델의 속도와 정확도를 직접 측정하여 최적의 선택을 해보세요.\n",
    "\n",
    "### 🏗️ YOLO 모델 패밀리\n",
    "\n",
    "**크기별 특징** 📏\n",
    "- **YOLOv8n (Nano)**: 약 6MB, 초고속 처리\n",
    "- **YOLOv8s (Small)**: 약 22MB, 빠른 속도 + 적당한 정확도\n",
    "- **YOLOv8m (Medium)**: 약 52MB, 균형잡힌 성능\n",
    "- **YOLOv8l (Large)**: 약 87MB, 높은 정확도 + 느린 속도\n",
    "\n",
    "**트레이드오프 관계** ⚖️\n",
    "- 모델이 클수록 → 더 정확하지만 더 느림\n",
    "- 모델이 작을수록 → 덜 정확하지만 더 빠름\n",
    "- 실제 사용시에는 목적에 맞는 균형점을 찾아야 함\n",
    "\n",
    "### 🎛️ 실시간 vs 정확도 선택\n",
    "\n",
    "**실시간 처리 우선시** ⚡\n",
    "- **용도**: 라이브 스트리밍, 실시간 모니터링\n",
    "- **목표 FPS**: 25+ FPS\n",
    "- **추천 모델**: YOLOv8n, YOLOv8s\n",
    "\n",
    "**정확도 우선시** 🎯\n",
    "- **용도**: 의료 진단, 보안 분석, 품질 검사\n",
    "- **허용 지연**: 1-3초\n",
    "- **추천 모델**: YOLOv8m, YOLOv8l\n",
    "\n",
    "### 🧪 성능 테스트 방법론\n",
    "\n",
    "**워밍업 (Warm-up)** 🔥\n",
    "- 첫 번째 실행은 항상 느림 (모델 로딩, GPU 초기화)\n",
    "- 정확한 측정을 위해 1번 실행 후 시간 측정 시작\n",
    "- 실제 서비스에서도 미리 워밍업하는 것이 일반적\n",
    "\n",
    "**반복 측정** 🔄\n",
    "- 10번 반복 실행으로 평균 시간 계산\n",
    "- 네트워크나 시스템 부하로 인한 변동성 제거\n",
    "- 더 신뢰할 만한 성능 데이터 확보\n",
    "\n",
    "**동일 조건 보장** 📐\n",
    "- 같은 테스트 이미지 사용\n",
    "- 같은 GPU/CPU 사용\n",
    "- 같은 신뢰도 설정값 사용\n",
    "\n",
    "### 📊 성능 지표 해석\n",
    "\n",
    "**처리 시간 (Processing Time)** ⏱️\n",
    "- 밀리초(ms) 단위로 측정\n",
    "- 0.033초 = 30 FPS (실시간 목표)\n",
    "- 0.050초 = 20 FPS (실용적 수준)\n",
    "- 0.100초 = 10 FPS (느리지만 사용 가능)\n",
    "\n",
    "**FPS 계산** 🎬"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af17821",
   "metadata": {},
   "source": [
    "### 🏆 모델 선택 가이드\n",
    "\n",
    "**상황별 추천** 💡\n",
    "\n",
    "**웹캠 실시간 데모**\n",
    "- YOLOv8n: 부드러운 30+ FPS 보장\n",
    "- 약간의 정확도 손실은 실시간성으로 커버\n",
    "\n",
    "**보안 카메라 시스템**\n",
    "- YOLOv8s: 15-25 FPS로 적당한 반응성\n",
    "- 사람 인식 정확도 향상\n",
    "\n",
    "**연구용 또는 오프라인 분석**\n",
    "- YOLOv8m: 높은 정확도로 세밀한 분석\n",
    "- 처리 시간보다 정확도가 중요\n",
    "\n",
    "### 🔧 GPU 메모리 고려사항\n",
    "\n",
    "**메모리 사용량** 💾\n",
    "- 큰 모델일수록 더 많은 GPU 메모리 사용\n",
    "- 듀얼 GPU 환경에서는 각각 다른 모델 동시 실행 가능\n",
    "- 메모리 부족시 배치 크기 조정 필요\n",
    "\n",
    "**동시 실행** 🔀\n",
    "- GPU 0: YOLOv8n (빠른 처리용)\n",
    "- GPU 1: YOLOv8m (정확한 분석용)\n",
    "- 실시간 + 정밀 분석 동시 수행 가능\n",
    "\n",
    "### 💼 실제 업계에서는?\n",
    "\n",
    "**상용 서비스 기준** 🏢\n",
    "- **Tesla Autopilot**: 초고속 추론 (수ms 이내)\n",
    "- **YouTube 자동 자막**: 실시간 처리 우선\n",
    "- **Google Photos**: 정확도 우선 (업로드 후 처리)\n",
    "\n",
    "**하드웨어별 최적화** 🖥️\n",
    "- **모바일**: 극도로 경량화된 모델\n",
    "- **서버**: 정확도 최우선 대형 모델\n",
    "- **엣지 디바이스**: 실시간 처리 가능한 중간 크기\n",
    "\n",
    "### 🎓 학습 포인트\n",
    "\n",
    "**이번 단계에서 배우는 것** 📚\n",
    "1. 동일 조건에서의 정확한 성능 비교 방법\n",
    "2. 실무에서 모델 선택시 고려해야 할 요소들\n",
    "3. GPU 성능과 모델 크기의 관계\n",
    "4. 실시간 처리의 기준과 임계값\n",
    "\n",
    "**다음 단계 예고** 🔮\n",
    "성능 테스트가 끝나면 고급 기능(신뢰도 조절, 필터링)이 추가된 실시간 인식을 경험해볼 예정입니다!\n",
    "\n",
    "이제 각 모델의 성능을 직접 측정하고 여러분만의 최적 모델을 찾아보세요! 🏁"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5156c715",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7단계: 다양한 YOLO 모델 성능 비교\n",
    "# =============================================================================\n",
    "\"\"\"\n",
    "일곱 번째 단계: 다양한 크기의 YOLO 모델을 비교해봅시다!\n",
    "크기가 클수록 정확하지만 느려집니다.\n",
    "\"\"\"\n",
    "\n",
    "# AI 모델 성능 측정 프로그램 시작을 알려줌\n",
    "print(\"⚖️  YOLO 모델 성능 비교\")\n",
    "# ⚖️ 저울 이모지로 공정한 성능 비교를 상징함\n",
    "# 여러 모델을 동일한 조건에서 측정해서 객관적으로 평가하는 과정\n",
    "\n",
    "print(\"=\"*25)  # 25개의 = 문자로 구분선을 만듦\n",
    "\n",
    "def compare_yolo_models():\n",
    "    \"\"\"다양한 YOLO 모델의 성능을 비교하는 함수\"\"\"\n",
    "    # 벤치마킹(성능 측정) 전용 함수\n",
    "    # 여러 AI 모델을 동일한 조건에서 테스트해서 최적 모델을 찾는 과정\n",
    "    \n",
    "    # 테스트할 세 가지 모델을 리스트로 정의함\n",
    "    # 튜플(파일명, 설명) 형태로 모델 정보를 저장함\n",
    "    models_to_test = [\n",
    "        ('yolov8n.pt', 'Nano - 가장 빠름'),      # n = nano, 가장 작은 모델, 실시간 처리 최적화\n",
    "        ('yolov8s.pt', 'Small - 균형'),          # s = small, 중간 크기, 속도와 정확도의 황금 비율\n",
    "        ('yolov8m.pt', 'Medium - 높은 정확도')   # m = medium, 큰 모델, 정확도 우선, 서버용\n",
    "    ]\n",
    "    # YOLO 모델 크기별 특징:\n",
    "    # nano < small < medium < large < xlarge 순으로 크기와 정확도가 증가함\n",
    "    # 크기가 클수록 더 많은 연산이 필요해서 속도는 느려짐\n",
    "    \n",
    "    # 웹캠에서 테스트용 이미지를 실시간으로 가져옴\n",
    "    if webcam is not None:  # 웹캠 객체가 제대로 초기화되었는지 확인함\n",
    "        ret, test_image = webcam.read()  # 웹캠에서 한 프레임(사진 한 장)을 읽어옴\n",
    "        # ret은 성공 여부(True/False), test_image는 실제 이미지 데이터(numpy 배열)\n",
    "        # 모든 모델에 동일한 이미지를 사용해야 공정한 비교가 가능함\n",
    "        \n",
    "        if not ret:  # 이미지 읽기에 실패했다면\n",
    "            print(\"❌ 테스트 이미지를 캡처할 수 없습니다!\")\n",
    "            return  # 함수를 즉시 종료하고 메인 프로그램으로 돌아감\n",
    "    else:  # 웹캠이 None이라면 (초기화 실패)\n",
    "        print(\"❌ 웹캠이 준비되지 않았습니다!\")\n",
    "        return  # 함수 종료, 웹캠 없이는 성능 테스트 불가능\n",
    "    \n",
    "    # 성능 테스트 과정을 사용자에게 친절하게 안내함\n",
    "    print(\"🧪 성능 테스트 시작...\")\n",
    "    # 🧪 실험 플라스크 이모지로 과학적 측정 과정임을 표현함\n",
    "    print(\"각 모델로 10번씩 처리하여 평균 시간을 측정합니다.\\n\")\n",
    "    # 왜 10번 반복하는가? 한 번만 측정하면 우연적 오차가 클 수 있음\n",
    "    # 여러 번 측정해서 평균을 내면 더 신뢰할 수 있는 결과를 얻음\n",
    "    \n",
    "    # 각 모델의 테스트 결과를 저장할 빈 리스트를 생성함\n",
    "    results = []\n",
    "    # 나중에 모든 결과를 비교하고 최적 모델을 찾기 위해 데이터를 보관함\n",
    "    \n",
    "    # models_to_test 리스트의 각 모델에 대해 반복 실행함\n",
    "    for model_name, description in models_to_test:\n",
    "        # 튜플 언패킹: (파일명, 설명)을 각각 변수에 할당함\n",
    "        \n",
    "        try:  # 오류가 발생할 수 있는 코드를 try 블록 안에 작성함\n",
    "            # 모델 파일이 없거나, GPU 메모리 부족 등의 문제 대응\n",
    "            print(f\"🔄 {model_name} ({description}) 테스트 중...\")\n",
    "            # 🔄 회전 화살표로 현재 진행 중인 작업을 표시함\n",
    "            \n",
    "            # 지정된 모델 파일을 불러와서 새로운 YOLO 객체를 생성함\n",
    "            test_model = YOLO(model_name)  # .pt 파일에서 학습된 AI 모델을 로드함\n",
    "            test_model.to(device)  # 모델을 GPU 또는 CPU로 이동시킴 (전역변수 device 사용)\n",
    "            # GPU가 있으면 CUDA로, 없으면 CPU로 자동 설정됨\n",
    "            \n",
    "            # 워밍업 실행: 첫 번째 처리는 항상 느리므로 측정에서 제외함\n",
    "            # AI 모델은 첫 실행 시 메모리 할당, 초기화 등으로 시간이 오래 걸림\n",
    "            _ = test_model(test_image, device=device, verbose=False)\n",
    "            # _은 결과를 사용하지 않는다는 의미의 파이썬 관례적 변수명\n",
    "            # verbose=False: 상세한 로그 출력을 비활성화함 (측정 방해 방지)\n",
    "            # 이 실행은 순수하게 모델을 준비 상태로 만들기 위한 목적\n",
    "            \n",
    "            # 실제 성능 측정을 시작함\n",
    "            start_time = time.time()  # 현재 시간을 초 단위로 기록함\n",
    "            # time.time()은 1970년 1월 1일부터의 경과 초를 반환함 (타임스탬프)\n",
    "            \n",
    "            # 정확한 평균값을 위해 동일한 작업을 10번 반복함\n",
    "            for _ in range(10):  # 0부터 9까지 10번 반복, 인덱스는 사용 안 함\n",
    "                _ = test_model(test_image, device=device, verbose=False)\n",
    "                # 매번 같은 이미지로 객체 탐지를 수행함\n",
    "                # 결과는 저장하지 않고 순수하게 처리 시간만 측정함\n",
    "            \n",
    "            end_time = time.time()  # 10번 처리 완료 후 시간을 기록함\n",
    "            \n",
    "            # 성능 지표를 계산함\n",
    "            avg_time = (end_time - start_time) / 10  # 총 소요시간을 10으로 나누어 평균을 계산함\n",
    "            # 예: 총 2초 걸렸다면 → 2/10 = 0.2초가 한 프레임 평균 처리 시간\n",
    "            \n",
    "            fps = 1 / avg_time  # FPS = 1초 / 한 프레임 처리시간\n",
    "            # 예: 0.1초에 1프레임 처리한다면 → 1/0.1 = 10 FPS\n",
    "            # FPS가 높을수록 더 빠른 모델임 (초당 더 많은 프레임 처리 가능)\n",
    "            \n",
    "            # 결과를 딕셔너리(사전) 형태로 정리해서 results 리스트에 추가함\n",
    "            results.append({\n",
    "                'model': model_name,        # 모델 파일명을 저장함\n",
    "                'description': description, # 모델 설명을 저장함\n",
    "                'avg_time': avg_time,      # 평균 처리 시간을 저장함 (초 단위)\n",
    "                'fps': fps                 # 초당 프레임 수를 저장함\n",
    "            })\n",
    "            # 딕셔너리로 저장하면 나중에 키 이름으로 쉽게 데이터에 접근할 수 있음\n",
    "            \n",
    "            # 각 모델의 개별 결과를 즉시 출력함 (사용자에게 진행 상황 알림)\n",
    "            print(f\"   ⏱️  평균 처리 시간: {avg_time:.3f}초\")  # :.3f는 소수점 3자리까지 표시\n",
    "            print(f\"   🎬 예상 FPS: {fps:.1f}\")              # :.1f는 소수점 1자리까지 표시\n",
    "            print()  # 빈 줄로 각 모델 결과를 구분해서 가독성을 높임\n",
    "            \n",
    "        except Exception as e:  # try 블록에서 어떤 오류든 발생하면 실행됨\n",
    "            # 모델 파일이 없거나, 메모리 부족, GPU 오류 등 다양한 문제가 가능함\n",
    "            print(f\"   ❌ 오류: {e}\")  # 구체적인 오류 메시지를 출력함\n",
    "            print()  # 빈 줄로 구분\n",
    "            # 한 모델에서 오류가 나도 다른 모델 테스트는 계속 진행함\n",
    "    \n",
    "    # 모든 모델 테스트 완료 후 결과를 표 형태로 정리해서 출력함\n",
    "    print(\"📋 성능 비교 결과:\")\n",
    "    # 📋 클립보드 이모지로 결과 요약 보고서임을 표현함\n",
    "    print(\"-\" * 50)  # 50개의 - 문자로 표 상단 구분선을 생성함\n",
    "    \n",
    "    # results 리스트의 각 딕셔너리에서 데이터를 꺼내서 깔끔한 표를 만듦\n",
    "    for result in results:\n",
    "        # 문자열 포맷팅으로 일정한 폭의 표를 만듦\n",
    "        print(f\"{result['model']:<12} | {result['description']:<15} | \"\n",
    "              f\"{result['fps']:>6.1f} FPS | {result['avg_time']:>6.3f}초\")\n",
    "        # 포맷팅 설명:\n",
    "        # <12: 왼쪽 정렬하고 최소 12칸을 확보함 (모델명 열)\n",
    "        # <15: 왼쪽 정렬하고 최소 15칸을 확보함 (설명 열)\n",
    "        # >6.1f: 오른쪽 정렬, 6칸 확보, 소수점 1자리 (FPS 열)\n",
    "        # >6.3f: 오른쪽 정렬, 6칸 확보, 소수점 3자리 (시간 열)\n",
    "        # | 문자로 열을 구분해서 표 형태로 만듦\n",
    "    \n",
    "    # 테스트 결과를 바탕으로 결론과 권장사항을 도출함\n",
    "    print(\"\\n💡 결론:\")\n",
    "    # 💡 전구 이모지로 중요한 인사이트임을 표시함\n",
    "    \n",
    "    if results:  # results 리스트가 비어있지 않다면 (테스트 결과가 하나라도 있다면)\n",
    "        # 람다 함수를 사용해서 FPS가 가장 높은 모델을 찾음\n",
    "        fastest = max(results, key=lambda x: x['fps'])\n",
    "        # lambda x: x['fps']는 각 딕셔너리에서 'fps' 값을 반환하는 함수\n",
    "        # max() 함수가 이 값들을 비교해서 가장 큰 값을 가진 딕셔너리를 반환함\n",
    "        # 즉, FPS가 가장 높은(가장 빠른) 모델을 찾는 과정\n",
    "        \n",
    "        print(f\"🏆 가장 빠른 모델: {fastest['model']} ({fastest['fps']:.1f} FPS)\")\n",
    "        # 🏆 트로피 이모지로 1위 모델임을 강조함\n",
    "        print(\"🎯 실시간 사용 권장: 30 FPS 이상인 모델\")\n",
    "        # 🎯 다트 이모지로 목표 기준을 제시함\n",
    "        # 30 FPS = 1초에 30프레임 처리 = 사람 눈에 자연스러운 영상\n",
    "        # 실시간 애플리케이션에서는 최소 30 FPS가 필요함\n",
    "\n",
    "# 위에서 정의한 모델 비교 함수를 실제로 실행함\n",
    "compare_yolo_models()\n",
    "# 이 한 줄로 전체 성능 비교 과정이 시작됨\n",
    "\n",
    "# 프로그램 섹션 마무리 구분선을 출력함\n",
    "print(\"\\n\" + \"=\"*50)  # 다음 단계와 구분하는 마무리 선\n",
    "# \\n으로 줄바꿈을 추가하고 = 문자 50개로 긴 구분선을 만듦\n",
    "\n",
    "# 💡 이 코드에서 배우는 중요한 개념들과 실무 활용법:\n",
    "# 1. 벤치마킹 - 여러 모델을 동일한 조건에서 공정하게 비교하는 과학적 방법\n",
    "# 2. 워밍업 - AI 모델 첫 실행의 지연시간을 제거해서 정확한 성능을 측정하는 기법\n",
    "# 3. 반복 측정 - 한 번만 측정하면 부정확하므로 여러 번 측정해서 평균값을 사용하는 통계적 접근\n",
    "# 4. 트레이드오프 - 속도가 빠르면 정확도가 떨어지고, 정확하면 속도가 느린 상충 관계 이해\n",
    "# 5. 실시간 기준 - 30 FPS는 사람이 자연스럽게 느끼는 영상 속도의 심리학적 기준점\n",
    "#\n",
    "# 🎯 실제 AI 서비스 개발 시 이런 성능 비교는 필수 과정임!\n",
    "# 📊 결과를 바탕으로 용도에 맞는 최적 모델을 선택할 수 있음!\n",
    "# 🏃‍♂️ 실시간 처리가 필요하면 빠른 모델, 정확도가 중요하면 큰 모델을 선택하는 전략적 판단!\n",
    "# 💰 비용도 고려 요소: 큰 모델일수록 더 비싼 GPU가 필요하고 전력 소모도 큼!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f3a2bd",
   "metadata": {},
   "source": [
    "## 🔧 8단계: 고급 실시간 객체 인식 (신뢰도 조절 + 필터링)\n",
    "\n",
    "### 🎯 학습 목표\n",
    "기본 실시간 인식에서 한 단계 더 나아가, 사용자가 직접 AI의 동작을 실시간으로 제어할 수 있는 고급 시스템을 만들어보겠습니다. 이는 실제 상용 AI 서비스에서 사용되는 핵심 기능들입니다.\n",
    "\n",
    "### 🎛️ 새로운 조작 시스템\n",
    "\n",
    "**실시간 설정 변경** ⚡\n",
    "- **'c' 키**: 신뢰도 임계값 증가 (0.1씩 상승)\n",
    "- **'v' 키**: 신뢰도 임계값 감소 (0.1씩 하락)\n",
    "- **'p' 키**: 사람만 표시 모드 토글\n",
    "- **'r' 키**: 모든 설정을 기본값으로 리셋\n",
    "- **'q' 키**: 프로그램 종료\n",
    "\n",
    "**기존 기능 유지** 🔄\n",
    "- **'s' 키**: 스크린샷 저장\n",
    "- **'f' 키**: FPS 정보 표시/숨기기\n",
    "\n",
    "### 🎯 신뢰도 임계값의 마법\n",
    "\n",
    "**신뢰도란 무엇인가?** 🤔\n",
    "- AI가 \"이것이 정말 사람이다!\"라고 확신하는 정도\n",
    "- 0.0 (0%) ~ 1.0 (100%) 사이의 값\n",
    "- 높을수록 확실한 것만 표시, 낮을수록 많은 것을 표시\n",
    "\n",
    "**실시간 조절의 효과** 📈  \n",
    "신뢰도 0.9 → 매우 확실한 객체만 (정확하지만 놓칠 수 있음)  \n",
    "신뢰도 0.7 → 대부분의 객체 (실용적 수준)  \n",
    "신뢰도 0.5 → 많은 객체 (기본 설정)  \n",
    "신뢰도 0.3 → 거의 모든 객체 (잘못된 인식 포함)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e387f9a",
   "metadata": {},
   "source": [
    "### 🔍 객체 필터링 시스템\n",
    "\n",
    "**사람만 표시 모드** 👤\n",
    "- 80가지 객체 중 'person' 클래스만 선별 표시\n",
    "- 사람 카운팅, 출입 통제, 보안 시스템에 활용\n",
    "- 배경의 복잡한 객체들을 무시하고 사람에만 집중\n",
    "\n",
    "**필터링의 실제 응용** 🏢\n",
    "- **매장 관리**: 고객 수 실시간 카운팅\n",
    "- **보안 시스템**: 출입자 감지 및 추적\n",
    "- **헬스케어**: 환자 낙상 감지 시스템\n",
    "- **스마트홈**: 거주자 활동 패턴 분석\n",
    "\n",
    "### 🎨 향상된 시각화\n",
    "\n",
    "**커스텀 색상 시스템** 🌈\n",
    "- 각 객체 클래스마다 고유한 색상 할당\n",
    "- 같은 종류의 객체는 항상 같은 색으로 표시\n",
    "- 빨강, 초록, 파랑, 노랑, 자주, 청록, 보라, 주황 등 8가지 기본 색상\n",
    "\n",
    "**정보 표시 개선** 📊\n",
    "- 배경 박스로 가독성 향상\n",
    "- 실시간 설정 상태 표시\n",
    "- 영어 텍스트로 폰트 문제 해결\n",
    "\n",
    "### ⚙️ 시스템 아키텍처\n",
    "\n",
    "**실시간 상호작용 루프** 🔄  \n",
    "프레임 캡처 → 2. AI 인식 → 3. 필터링 적용 → 시각화 → 5. 사용자 입력 확인 → 6. 설정 업데이트 → 1번으로"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3200fc03",
   "metadata": {},
   "source": [
    "**성능 최적화** 🚀\n",
    "- 키 입력은 1ms만 대기 (실시간성 유지)\n",
    "- 설정 변경시에만 화면 업데이트\n",
    "- GPU 메모리 효율적 사용\n",
    "\n",
    "### 🧪 실험해볼 것들\n",
    "\n",
    "**신뢰도 실험** 🔬\n",
    "1. 신뢰도를 0.9로 올려보세요 → 확실한 객체만 남음\n",
    "2. 신뢰도를 0.3으로 내려보세요 → 많은 오탐지 발생\n",
    "3. 최적의 신뢰도 값을 찾아보세요\n",
    "\n",
    "**필터링 실험** 👥\n",
    "1. 사람만 표시 모드로 전환\n",
    "2. 여러 사람이 있는 환경에서 테스트\n",
    "3. 사람이 없을 때와 있을 때 차이 관찰\n",
    "\n",
    "**성능 실험** ⏱️\n",
    "1. 복잡한 장면에서 FPS 변화 관찰\n",
    "2. 필터링 적용시 성능 변화 측정\n",
    "3. 신뢰도 변경시 처리 속도 차이\n",
    "\n",
    "### 💡 실제 개발에서의 응용\n",
    "\n",
    "**동적 임계값 조정** 📈\n",
    "- 낮 시간대: 높은 신뢰도 (정확성 우선)\n",
    "- 밤 시간대: 낮은 신뢰도 (민감도 우선)\n",
    "- 환경에 따른 자동 조정 시스템\n",
    "\n",
    "**다중 필터 시스템** 🔧\n",
    "- 시간대별 관심 객체 변경\n",
    "- 구역별 다른 감지 설정\n",
    "- 사용자 권한에 따른 표시 제어\n",
    "\n",
    "### 🎓 학습 포인트\n",
    "\n",
    "**AI 시스템 설계 원칙** 📚\n",
    "1. **유연성**: 다양한 상황에 대응 가능\n",
    "2. **직관성**: 사용자가 쉽게 이해하고 조작\n",
    "3. **실시간성**: 즉각적인 피드백과 반응\n",
    "4. **안정성**: 오류 상황에서도 안전한 동작\n",
    "\n",
    "**사용자 경험 (UX) 고려사항** 💫\n",
    "- 명확한 시각적 피드백\n",
    "- 일관된 키보드 단축키\n",
    "- 직관적인 설정 변경 방식\n",
    "- 현재 상태의 명확한 표시\n",
    "\n",
    "### 🚀 다음 단계 예고\n",
    "\n",
    "이 고급 시스템을 마스터하면, 다음에는 듀얼 GPU를 활용한 초고성능 처리를 경험해볼 예정입니다. 두 개의 GPU로 서로 다른 모델을 동시에 실행하여 비교하는 놀라운 데모를 만나보세요!\n",
    "\n",
    "이제 AI의 \"뇌\"를 직접 조작하는 신기한 경험을 시작해보겠습니다! 🧠✨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b739b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 8단계: 고급 실시간 객체 인식 (신뢰도 조절 + 필터링)\n",
    "# =============================================================================\n",
    "\n",
    "\"\"\"\n",
    "여덟 번째 단계: 고급 기능이 추가된 실시간 인식을 해봅시다!\n",
    "신뢰도를 조절하고 특정 객체만 표시할 수 있습니다.\n",
    "\"\"\"\n",
    "# 이전 단계의 기본 실시간 인식에서 한 단계 더 발전한 고급 버전\n",
    "# 사용자가 실시간으로 AI 설정을 조절할 수 있는 인터랙티브 시스템\n",
    "# 실무에서 사용되는 AI 애플리케이션의 핵심 기능들을 구현함\n",
    "\n",
    "def advanced_real_time_detection():\n",
    "    \"\"\"고급 실시간 객체 인식 함수 (사용자 제어 기능 포함)\"\"\"\n",
    "    # 실시간 AI 제어 시스템의 모든 기능을 담은 고급 함수\n",
    "    # 기본 인식에서 사용자 맞춤형 설정과 실시간 조작 기능을 추가함\n",
    "    # 실제 상용 AI 서비스에서 볼 수 있는 사용자 경험을 구현함\n",
    "    \n",
    "    # 웹캠 준비 상태를 먼저 확인함\n",
    "    if webcam is None:\n",
    "        # 이전 단계에서 웹캠 초기화가 실패했다면 None 상태\n",
    "        print(\"❌ 웹캠이 준비되지 않았습니다!\")\n",
    "        return  # 웹캠 없이는 실시간 인식이 불가능하므로 함수 종료\n",
    "    \n",
    "    # 사용자가 실시간으로 조절할 수 있는 설정 변수들을 초기화함\n",
    "    confidence_threshold = 0.5  # 신뢰도 임계값 (기본값 50%)\n",
    "    # AI가 몇 퍼센트 이상 확신할 때만 객체로 인정할지 결정하는 기준\n",
    "    # 높을수록 확실한 것만 표시, 낮을수록 의심스러운 것도 표시\n",
    "    \n",
    "    person_only = False         # 사람만 표시 모드 (기본값: 모든 객체)\n",
    "    # True이면 사람(person)만 표시, False면 모든 객체 표시\n",
    "    # 보안 시스템이나 인원 카운팅 등에 유용한 기능\n",
    "    \n",
    "    frame_count = 0             # 처리한 프레임 수를 카운트하는 변수\n",
    "    # FPS 계산과 성능 모니터링에 사용됨\n",
    "    \n",
    "    start_time = time.time()    # 시작 시간을 기록함 (FPS 계산용)\n",
    "    # 프로그램 시작 시점의 타임스탬프를 저장해서 경과 시간 계산 기준점으로 사용\n",
    "    \n",
    "    # 각 객체 클래스별로 고유한 색상을 정의함 (BGR 순서로 OpenCV 형식)\n",
    "    colors = [\n",
    "        (255, 0, 0),    # 빨강 (Blue=255, Green=0, Red=0) - OpenCV는 BGR 순서임!\n",
    "        (0, 255, 0),    # 초록 (Blue=0, Green=255, Red=0)\n",
    "        (0, 0, 255),    # 파랑 (Blue=0, Green=0, Red=255)\n",
    "        (255, 255, 0),  # 노랑 (청록색으로 보임)\n",
    "        (255, 0, 255),  # 자주 (마젠타색)\n",
    "        (0, 255, 255),  # 청록 (시안색)\n",
    "        (128, 0, 128),  # 보라색\n",
    "        (255, 165, 0)   # 주황색\n",
    "    ]\n",
    "    # 왜 색상을 미리 정의하는가?\n",
    "    # 같은 종류의 객체는 항상 같은 색으로 표시해서 사용자가 구분하기 쉽게 함\n",
    "    # 예: 사람은 항상 빨강, 자동차는 항상 초록 등\n",
    "    \n",
    "    # 고급 실시간 인식 시작을 알림\n",
    "    print(f\"🚀 고급 실시간 인식 시작! (신뢰도: {confidence_threshold})\")\n",
    "    # 🚀 로켓 이모지로 고급 기능의 시작을 강조함\n",
    "    # 현재 신뢰도 설정을 함께 표시해서 사용자가 현재 상태를 알 수 있게 함\n",
    "    \n",
    "    try:  # 실시간 처리 중 발생할 수 있는 모든 오류에 대비한 예외 처리\n",
    "        # 웹캠 오류, 키보드 인터럽트, 메모리 부족 등에 안전하게 대응함\n",
    "        \n",
    "        while True:  # 실시간 처리를 위한 무한 루프\n",
    "            # 연속적인 프레임 처리로 실시간 영상 스트림을 만듦\n",
    "            # break 문을 만나기 전까지 계속 실행됨\n",
    "            \n",
    "            # 웹캠에서 한 프레임을 읽어옴\n",
    "            ret, frame = webcam.read()\n",
    "            # ret: 프레임 읽기 성공 여부 (True/False)\n",
    "            # frame: 실제 이미지 데이터 (numpy 배열 형태)\n",
    "            \n",
    "            if not ret:  # 프레임 읽기에 실패한 경우\n",
    "                # 웹캠 연결 끊김, 드라이버 문제 등이 원인일 수 있음\n",
    "                break    # while 루프를 종료해서 프로그램 안전하게 마무리\n",
    "            \n",
    "            # YOLO 모델로 객체 인식을 수행함 (사용자 설정 신뢰도 적용)\n",
    "            results = model(frame, device=device, conf=confidence_threshold, verbose=False)\n",
    "            # 매개변수 설명:\n",
    "            # frame: 방금 웹캠에서 읽어온 현재 프레임\n",
    "            # device: GPU 또는 CPU (이전에 설정된 전역 변수)\n",
    "            # conf=confidence_threshold: 사용자가 실시간으로 조절한 신뢰도 임계값 적용\n",
    "            # verbose=False: 상세 로그 출력 비활성화 (실시간 성능 향상)\n",
    "            \n",
    "            # 커스텀 결과 그리기를 위해 원본 프레임을 복사함\n",
    "            annotated_frame = frame.copy()\n",
    "            # 원본 프레임을 보존하고 복사본에 그림을 그려서 원본 손상 방지\n",
    "            # AI 결과를 직접 원본에 그리면 다음 프레임 처리에 방해될 수 있음\n",
    "            \n",
    "            # 인식된 모든 객체에 대해 하나씩 처리함\n",
    "            for result in results:\n",
    "                # results는 보통 1개 요소를 가진 리스트 (1개 이미지 처리 결과)\n",
    "                boxes = result.boxes  # 인식된 객체들의 바운딩 박스 정보를 가져옴\n",
    "                # boxes에는 각 객체의 위치, 클래스, 신뢰도 정보가 들어있음\n",
    "                \n",
    "                if boxes is not None:  # 객체가 하나라도 인식되었다면\n",
    "                    # boxes가 None이면 아무 객체도 발견되지 않은 상태\n",
    "                    \n",
    "                    for box in boxes:  # 인식된 각 객체에 대해 반복 처리\n",
    "                        # 객체의 상세 정보를 추출함\n",
    "                        class_id = int(box.cls[0])           # 클래스 ID 번호 (0=person, 1=bicycle, 2=car, ...)\n",
    "                        class_name = model.names[class_id]   # 클래스 이름을 문자열로 변환 ('person', 'car', ...)\n",
    "                        confidence = float(box.conf[0])      # 신뢰도 점수 (0.0~1.0 범위)\n",
    "                        \n",
    "                        # 사람만 표시 모드일 때 필터링을 적용함\n",
    "                        if person_only and class_name != 'person':\n",
    "                            continue  # 'person'이 아닌 객체는 건너뛰고 다음 객체로 이동\n",
    "                        # 보안 시스템이나 인원 카운팅에서 유용한 필터링 기능\n",
    "                        \n",
    "                        # 바운딩 박스의 좌표를 추출함 (x1,y1=좌상단, x2,y2=우하단)\n",
    "                        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                        # xyxy 형식: [x1, y1, x2, y2] (좌상단과 우하단 좌표)\n",
    "                        # map(int, ...): 실수 좌표를 정수로 변환 (픽셀 좌표는 정수여야 함)\n",
    "                        \n",
    "                        # 클래스 ID에 따른 색상을 선택함 (순환 방식)\n",
    "                        color = colors[class_id % len(colors)]\n",
    "                        # % 연산자로 colors 배열 크기를 넘지 않게 순환시킴\n",
    "                        # 예: class_id가 10이고 colors가 8개면 10%8=2번째 색상 사용\n",
    "                        # 이렇게 하면 객체 종류가 많아도 색상이 부족하지 않음\n",
    "                        \n",
    "                        # 객체 주변에 바운딩 박스(사각형 테두리)를 그림\n",
    "                        cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)\n",
    "                        # (x1, y1): 좌상단 좌표, (x2, y2): 우하단 좌표\n",
    "                        # color: 위에서 선택한 클래스별 고유 색상\n",
    "                        # 2: 테두리 선 두께 (픽셀 단위)\n",
    "                        \n",
    "                        # 라벨 텍스트를 준비함 (객체명 + 신뢰도 퍼센트)\n",
    "                        label = f\"{class_name}: {confidence:.2f}\"\n",
    "                        # 예: \"person: 0.85\" (85% 확신으로 사람 인식)\n",
    "                        # :.2f는 신뢰도를 소수점 2자리까지 표시\n",
    "                        \n",
    "                        # 라벨 텍스트의 크기를 미리 측정함\n",
    "                        label_size, _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2)\n",
    "                        # 텍스트 배경 박스 크기를 계산하기 위해 필요\n",
    "                        # label_size[0]: 텍스트 폭, label_size[1]: 텍스트 높이\n",
    "                        \n",
    "                        # 라벨 배경 박스를 그림 (텍스트 가독성 향상)\n",
    "                        cv2.rectangle(annotated_frame, (x1, y1 - label_size[1] - 10), \n",
    "                                     (x1 + label_size[0], y1), color, -1)\n",
    "                        # 바운딩 박스 위쪽에 텍스트 배경 생성\n",
    "                        # -10: 텍스트와 바운딩 박스 사이 여백\n",
    "                        # -1: 사각형 내부를 색상으로 채움 (속이 꽉 찬 박스)\n",
    "                        \n",
    "                        # 라벨 텍스트를 그림 (흰색 글씨로 명확하게 표시)\n",
    "                        cv2.putText(annotated_frame, label, (x1, y1 - 5),\n",
    "                                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "                        # (x1, y1 - 5): 바운딩 박스 바로 위에 텍스트 위치\n",
    "                        # (255, 255, 255): 흰색 (BGR 형식)\n",
    "                        # 2: 글자 두께\n",
    "            \n",
    "            # 성능 및 현재 설정 정보를 계산함\n",
    "            frame_count += 1  # 성공적으로 처리된 프레임 수를 1 증가\n",
    "            elapsed_time = time.time() - start_time  # 프로그램 시작부터의 경과 시간\n",
    "            fps = frame_count / elapsed_time if elapsed_time > 0 else 0  # 실시간 FPS 계산\n",
    "            # 0으로 나누는 오류를 방지하는 조건부 계산\n",
    "            \n",
    "            # 화면에 표시할 정보들을 준비함 (영어로 작성해서 폰트 문제 해결)\n",
    "            info_texts = [\n",
    "                f'FPS: {fps:.1f}',  # 현재 프레임 레이트 (실시간 성능 지표)\n",
    "                f'Confidence: {confidence_threshold:.2f}',  # 현재 신뢰도 설정값\n",
    "                f'Mode: {\"Person Only\" if person_only else \"All Objects\"}',  # 현재 필터링 모드\n",
    "                f'Device: {device}'  # 사용 중인 처리 장치 (GPU/CPU)\n",
    "            ]\n",
    "            # 사용자가 현재 설정 상태를 실시간으로 확인할 수 있게 함\n",
    "            \n",
    "            # 정보 표시용 배경 박스를 그림 (가독성 향상)\n",
    "            cv2.rectangle(annotated_frame, (5, 5), (350, 120), (0, 0, 0), -1)  # 검은색 배경\n",
    "            # (5, 5): 좌상단, (350, 120): 우하단, (0, 0, 0): 검은색, -1: 채움\n",
    "            cv2.rectangle(annotated_frame, (5, 5), (350, 120), (0, 255, 0), 2)  # 녹색 테두리\n",
    "            # 검은 배경 위에 녹색 테두리로 정보 영역을 명확히 구분\n",
    "            \n",
    "            # 정보 텍스트들을 한 줄씩 차례로 표시함\n",
    "            for i, text in enumerate(info_texts):\n",
    "                # enumerate로 인덱스와 텍스트를 동시에 가져옴\n",
    "                cv2.putText(annotated_frame, text, (10, 25 + i * 22), \n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
    "                # y 좌표 계산: 25 + i * 22 (첫 줄 25픽셀, 이후 22픽셀씩 아래로)\n",
    "                # (0, 255, 0): 녹색 텍스트\n",
    "            \n",
    "            # 완성된 프레임을 화면에 표시함\n",
    "            cv2.imshow('YOLO Advanced Real-time Detection', annotated_frame)\n",
    "            # 창 제목으로 고급 버전임을 표시\n",
    "            # annotated_frame: AI 결과, 설정 정보가 모두 포함된 최종 이미지\n",
    "            \n",
    "            # 키보드 입력을 확인하고 처리함 (1ms 대기로 실시간성 유지)\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            # 1ms만 대기해서 실시간 처리에 방해되지 않게 함\n",
    "            # & 0xFF: 8비트로 제한해서 특수키 문제 해결\n",
    "            \n",
    "            # 각 키별로 다른 기능을 실행함 (사용자 인터페이스)\n",
    "            if key == ord('q'):  # 'q' 키: 프로그램 종료\n",
    "                print(\"👋 프로그램을 종료합니다!\")\n",
    "                break  # while 루프 종료\n",
    "                \n",
    "            elif key == ord('c'):  # 'c' 키: 신뢰도 증가 (Confidence up)\n",
    "                confidence_threshold = min(0.9, confidence_threshold + 0.1)  # 최대 0.9(90%)까지\n",
    "                # min 함수로 상한선 0.9를 넘지 않게 제한\n",
    "                print(f\"🔺 Confidence increased: {confidence_threshold:.2f}\")\n",
    "                # 🔺 위쪽 삼각형으로 증가를 표현\n",
    "                \n",
    "            elif key == ord('v'):  # 'v' 키: 신뢰도 감소 (Decrease, c 아래 키)\n",
    "                confidence_threshold = max(0.1, confidence_threshold - 0.1)  # 최소 0.1(10%)까지\n",
    "                # max 함수로 하한선 0.1을 밑돌지 않게 제한\n",
    "                print(f\"🔻 Confidence decreased: {confidence_threshold:.2f}\")\n",
    "                # 🔻 아래쪽 삼각형으로 감소를 표현\n",
    "                \n",
    "            elif key == ord('p'):  # 'p' 키: 사람만 표시 모드 토글 (Person only)\n",
    "                person_only = not person_only  # 불린 값 반전 (True ↔ False)\n",
    "                # 토글 방식으로 켜고 끄기를 반복할 수 있음\n",
    "                print(f\"👤 Person only mode: {'ON' if person_only else 'OFF'}\")\n",
    "                # 👤 사람 아이콘으로 기능을 직관적으로 표현\n",
    "                \n",
    "            elif key == ord('r'):  # 'r' 키: 설정 리셋 (Reset)\n",
    "                confidence_threshold = 0.5  # 기본 신뢰도(50%)로 복원\n",
    "                person_only = False         # 모든 객체 표시 모드로 복원\n",
    "                print(\"🔄 Settings reset!\")\n",
    "                # 🔄 새로고침 아이콘으로 리셋을 표현\n",
    "                # 사용자가 설정을 잘못 조절했을 때 쉽게 원래대로 돌리기 가능\n",
    "    \n",
    "    except KeyboardInterrupt:  # Ctrl+C로 강제 종료했을 때\n",
    "        print(\"\\n⚠️  사용자에 의해 중단됨\")\n",
    "        # ⚠️ 경고 표시로 비정상 종료가 아닌 의도적 중단임을 알림\n",
    "    \n",
    "    finally:  # 정상 종료든 오류 종료든 반드시 실행되는 정리 코드\n",
    "        cv2.destroyAllWindows()  # 모든 OpenCV 창을 닫고 메모리에서 제거\n",
    "        # GUI 자원을 해제해서 시스템 안정성 확보\n",
    "        \n",
    "        # 최종 성능 통계를 출력함\n",
    "        final_fps = frame_count / (time.time() - start_time) if frame_count > 0 else 0\n",
    "        # 전체 세션의 평균 FPS를 계산함\n",
    "        print(f\"📊 최종 평균 FPS: {final_fps:.2f}\")\n",
    "        # 📊 차트 아이콘으로 통계 정보임을 표현\n",
    "\n",
    "# 고급 실시간 인식 함수를 실행함\n",
    "print(\"💡 고급 실시간 인식 조작법:\")\n",
    "print(\"   🔤 키보드 조작:\")\n",
    "print(\"   - 'q': 종료\")\n",
    "print(\"   - 'c': 신뢰도 증가 (더 까다롭게)\")\n",
    "print(\"   - 'v': 신뢰도 감소 (더 관대하게)\")  \n",
    "print(\"   - 'p': 사람만 표시 모드 토글\")\n",
    "print(\"   - 'r': 모든 설정 초기화\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# 함수 실행\n",
    "advanced_real_time_detection()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# 💡 이 코드에서 배우는 고급 개념들과 실무 활용법:\n",
    "# 1. 실시간 파라미터 조정 - AI의 민감도를 사용자가 즉석에서 변경하는 인터랙티브 시스템\n",
    "# 2. 객체 필터링 시스템 - 관심 있는 특정 객체만 선별적으로 표시하는 맞춤형 기능\n",
    "# 3. 커스텀 색상 시스템 - 각 객체 종류별로 고유 색상을 부여해서 직관적 구분 가능\n",
    "# 4. 사용자 인터페이스 설계 - 직관적인 키보드 조작으로 복잡한 AI 설정을 쉽게 제어\n",
    "# 5. 실시간 피드백 시스템 - 설정 변경 시 즉각적인 화면 반영과 상태 알림\n",
    "# 6. 안전한 범위 제한 - min/max 함수로 설정값이 유효 범위를 벗어나지 않게 보호\n",
    "# 7. 토글 방식 UI - 한 키로 기능 켜기/끄기를 반복할 수 있는 편리한 조작\n",
    "#\n",
    "# 🎯 실제 AI 서비스에서 이런 실시간 제어 기능들이 핵심 차별화 요소임!\n",
    "# 🏢 보안 시스템: 사람만 감지, 신뢰도 조절로 오탐지 방지\n",
    "# 🚗 자율주행: 날씨/조명에 따른 실시간 민감도 조절\n",
    "# 🏭 제조업: 불량품 검출 기준을 실시간으로 조정\n",
    "# 📱 모바일 앱: 사용자 선호에 맞는 개인화된 AI 설정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c56425",
   "metadata": {},
   "source": [
    "## 🚀 9단계: 듀얼 GPU 고성능 처리\n",
    "\n",
    "### 🎯 학습 목표\n",
    "이제 우리 컴퓨터의 진정한 파워를 발휘할 시간입니다! 두 개의 GPU를 동시에 활용하여 서로 다른 YOLO 모델을 실행하고, 그 결과를 실시간으로 비교해보겠습니다. 이는 최첨단 AI 연구소에서나 볼 수 있는 고급 기술입니다.\n",
    "\n",
    "### 🔥 듀얼 GPU의 위력\n",
    "\n",
    "**병렬 처리의 혁신** ⚡\n",
    "- **GPU 0**: YOLOv8n (초고속 처리) - 50+ FPS 달성\n",
    "- **GPU 1**: YOLOv8m (정밀 분석) - 25+ FPS 달성  \n",
    "- 두 모델이 **동시에** 같은 영상을 처리합니다!\n",
    "\n",
    "**실시간 비교 분석** 📊\n",
    "- 같은 프레임을 두 모델이 다르게 인식하는 차이점 관찰\n",
    "- 빠른 모델 vs 정확한 모델의 실시간 대결\n",
    "- 좌우 분할 화면으로 직관적 비교 가능\n",
    "\n",
    "### 🎭 모델별 특성 비교\n",
    "\n",
    "**YOLOv8n (GPU 0) - 스피드 챔피언** 🏃‍♂️\n",
    "- **장점**: 초고속 처리, 실시간성 완벽 보장\n",
    "- **단점**: 작은 객체나 멀리 있는 객체 놓칠 수 있음\n",
    "- **용도**: 실시간 스트리밍, 라이브 모니터링\n",
    "\n",
    "**YOLOv8m (GPU 1) - 정확도 마스터** 🎯\n",
    "- **장점**: 높은 정확도, 세밀한 객체까지 감지\n",
    "- **단점**: 상대적으로 느린 처리 속도\n",
    "- **용도**: 정밀 분석, 보안 시스템, 의료 진단\n",
    "\n",
    "### 🖥️ 듀얼 디스플레이 시스템\n",
    "\n",
    "**화면 구성** 📺\n",
    "\n",
    "\n",
    "| GPU 0 결과 | GPU 1 결과 |\n",
    "|:----------:|:----------:|\n",
    "| **YOLOv8n** | **YOLOv8m** |\n",
    "| ⚡ 빠른 처리 | 🎯 정확한 처리 |\n",
    "| 50+ FPS | 25+ FPS |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc933e4",
   "metadata": {},
   "source": [
    "**시각적 비교 요소** 👀\n",
    "- 같은 객체를 두 모델이 다른 신뢰도로 인식\n",
    "- 한 모델은 놓치고 다른 모델은 찾는 객체들\n",
    "- 처리 속도의 실시간 차이 체감\n",
    "\n",
    "### ⚙️ 고급 GPU 관리\n",
    "\n",
    "**메모리 분산 처리** 💾\n",
    "- 각 GPU가 독립적인 메모리 공간 사용\n",
    "- 메모리 충돌 없이 안정적인 동시 처리\n",
    "- 전체 시스템 성능 최적화\n",
    "\n",
    "**동기화 처리** 🔄\n",
    "- 두 GPU의 결과를 정확히 같은 타이밍에 표시\n",
    "- 프레임 드롭 최소화\n",
    "- 자연스러운 좌우 분할 화면 구현\n",
    "\n",
    "### 🎮 조작 및 모니터링\n",
    "\n",
    "**키보드 조작** ⌨️\n",
    "- **'s' 키**: 듀얼 GPU 스크린샷 저장\n",
    "- **'q' 키**: 프로그램 종료\n",
    "- **실시간 FPS 모니터링**: 각 GPU별 성능 표시\n",
    "\n",
    "**성능 메트릭** 📈\n",
    "- **개별 FPS**: 각 GPU의 독립적인 처리 속도\n",
    "- **통합 FPS**: 전체 시스템의 효율성\n",
    "- **GPU 사용률**: 하드웨어 활용도 실시간 확인\n",
    "\n",
    "### 💡 실제 응용 사례\n",
    "\n",
    "**상용 서비스에서의 듀얼 GPU** 🏢\n",
    "- **Tesla 자율주행**: 여러 센서 데이터 동시 처리\n",
    "- **Netflix 스트리밍**: 실시간 인코딩 + 품질 최적화\n",
    "- **Google 검색**: 이미지 인식 + 텍스트 분석 병렬 처리\n",
    "\n",
    "**연구 개발 환경** 🔬\n",
    "- 새로운 모델과 기존 모델의 실시간 성능 비교\n",
    "- A/B 테스트를 통한 알고리즘 개선\n",
    "- 다양한 설정값의 동시 실험\n",
    "\n",
    "### 🛠️ 기술적 도전과 해결\n",
    "\n",
    "**동기화 문제** ⏰\n",
    "- 두 GPU의 처리 속도가 다를 때 화면 동기화\n",
    "- 프레임 버퍼링으로 자연스러운 표시\n",
    "\n",
    "**메모리 관리** 💾\n",
    "- 각 GPU별 최적 메모리 할당\n",
    "- 메모리 누수 방지 및 효율적 정리\n",
    "\n",
    "**열 관리** 🌡️\n",
    "- 두 GPU 동시 사용시 발열 증가\n",
    "- 적절한 쿨링과 성능 모니터링 필요\n",
    "\n",
    "### 🎓 학습 가치\n",
    "\n",
    "**고급 시스템 아키텍처** 🏗️\n",
    "- 병렬 처리 시스템의 설계 원리\n",
    "- 하드웨어 자원의 효율적 활용\n",
    "- 확장 가능한 AI 시스템 구축\n",
    "\n",
    "**실무 역량 개발** 💪\n",
    "- 엔터프라이즈급 AI 시스템 경험\n",
    "- 성능 최적화 및 벤치마킹 기법\n",
    "- 멀티 GPU 환경에서의 개발 노하우\n",
    "\n",
    "### ⚠️ 시스템 요구사항\n",
    "\n",
    "**하드웨어 조건** 🖥️\n",
    "- NVIDIA GPU 2개 이상 필수\n",
    "- 충분한 전력 공급 (750W+ PSU 권장)\n",
    "- 적절한 냉각 시스템\n",
    "\n",
    "**소프트웨어 조건** 💻\n",
    "- CUDA 11.8 이상\n",
    "- 최신 GPU 드라이버\n",
    "- 충분한 시스템 RAM (16GB+)\n",
    "\n",
    "### 🚨 주의사항\n",
    "\n",
    "**전력 소비** ⚡\n",
    "- 두 GPU 동시 사용시 전력 소비 급증\n",
    "- 시스템 안정성 모니터링 필요\n",
    "\n",
    "**열 발생** 🔥\n",
    "- GPU 온도 상승 주의\n",
    "- 장시간 사용시 휴식 필요\n",
    "\n",
    "이제 정말 프로페셔널한 AI 워크스테이션의 성능을 체험해보세요! 마치 SF 영화에서나 볼 법한 고성능 AI 시스템을 직접 조작하는 짜릿한 경험이 기다리고 있습니다! 🌟🚀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f47b9179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 9단계: 듀얼 GPU 고성능 처리\n",
    "# =============================================================================\n",
    "\n",
    "\"\"\"\n",
    "아홉 번째 단계: 듀얼 GPU를 활용한 고성능 처리를 해봅시다!\n",
    "두 개의 GPU를 동시에 사용해서 더 빠른 처리가 가능합니다.\n",
    "\"\"\"\n",
    "# 이 단계는 AI 분야의 최고급 기술 중 하나인 멀티 GPU 병렬 처리를 다룸\n",
    "# 실제 AI 연구소나 대기업에서 사용하는 고성능 컴퓨팅 기법\n",
    "# 한 대의 GPU로는 한계가 있는 복잡한 AI 작업을 여러 GPU로 분산 처리함\n",
    "\n",
    "# 듀얼 GPU 처리 시작을 알림\n",
    "print(\"🚀 듀얼 GPU 고성능 처리\")\n",
    "# 🚀 로켓 이모지로 고성능 처리의 혁신성을 강조함\n",
    "print(\"=\"*23)  # 23개의 = 문자로 구분선 생성\n",
    "\n",
    "def dual_gpu_demonstration():\n",
    "    \"\"\"듀얼 GPU를 활용한 고성능 데모 함수\"\"\"\n",
    "    # 멀티 GPU 시스템의 핵심 개념과 구현 방법을 보여주는 교육용 함수\n",
    "    # 실무에서 사용되는 분산 처리 기법을 간단한 예제로 체험할 수 있음\n",
    "    # 두 개의 GPU가 서로 다른 AI 모델을 동시에 실행하는 혁신적 접근\n",
    "    \n",
    "    # 듀얼 GPU 환경을 확인함 (최소 2개 GPU 필요)\n",
    "    if torch.cuda.device_count() < 2:\n",
    "        # torch.cuda.device_count(): 시스템에서 사용 가능한 CUDA GPU 개수를 반환함\n",
    "        # CUDA는 NVIDIA GPU를 프로그래밍하기 위한 플랫폼\n",
    "        # 2개 미만이면 듀얼 GPU 처리가 불가능함\n",
    "        \n",
    "        print(\"⚠️  듀얼 GPU가 감지되지 않았습니다.\")\n",
    "        print(f\"   현재 GPU 개수: {torch.cuda.device_count()}\")\n",
    "        # 현재 시스템의 GPU 개수를 표시해서 사용자가 현황을 파악할 수 있게 함\n",
    "        print(\"   이 단계는 건너뛰겠습니다.\")\n",
    "        return  # GPU가 2개 미만이면 함수를 종료함\n",
    "        # 하드웨어 요구사항을 만족하지 않으면 안전하게 종료\n",
    "    \n",
    "    # 듀얼 GPU 환경 확인 완료를 알림\n",
    "    print(\"🔥 듀얼 GPU 환경 감지!\")\n",
    "    # 🔥 불꽃 이모지로 강력한 하드웨어 성능을 표현함\n",
    "    \n",
    "    print(f\"GPU 0: {torch.cuda.get_device_name(0)}\")  # 첫 번째 GPU의 정확한 모델명\n",
    "    print(f\"GPU 1: {torch.cuda.get_device_name(1)}\")  # 두 번째 GPU의 정확한 모델명\n",
    "    # get_device_name(): 각 GPU의 상세 모델명을 반환함 (예: \"GeForce RTX 4090\")\n",
    "    # 사용자가 어떤 GPU를 가지고 있는지 명확히 알 수 있음\n",
    "    print()  # 빈 줄로 섹션을 구분함\n",
    "    \n",
    "    # 각각 다른 GPU에 서로 다른 YOLO 모델을 로드함\n",
    "    print(\"🤖 각 GPU에 다른 모델 로딩...\")\n",
    "    # 🤖 로봇 이모지로 AI 모델 로딩 과정을 표현함\n",
    "    \n",
    "    model_gpu0 = YOLO('yolov8n.pt').to('cuda:0')  # GPU 0에 nano 모델 (빠른 처리용)\n",
    "    # YOLO('yolov8n.pt'): nano 크기 모델을 로드함 (가장 빠르지만 정확도는 상대적으로 낮음)\n",
    "    # .to('cuda:0'): 첫 번째 GPU (인덱스 0)에 모델을 배치함\n",
    "    # 실시간 처리가 중요한 용도에 최적화된 선택\n",
    "    \n",
    "    model_gpu1 = YOLO('yolov8m.pt').to('cuda:1')  # GPU 1에 medium 모델 (정확한 처리용)\n",
    "    # YOLO('yolov8m.pt'): medium 크기 모델을 로드함 (nano보다 크고 정확하지만 느림)\n",
    "    # .to('cuda:1'): 두 번째 GPU (인덱스 1)에 모델을 배치함\n",
    "    # 정확도가 중요한 용도에 최적화된 선택\n",
    "    \n",
    "    # 이렇게 역할을 분담하는 이유:\n",
    "    # GPU 0은 실시간성이 중요한 작업 (예: 실시간 모니터링)\n",
    "    # GPU 1은 정확도가 중요한 작업 (예: 정밀 분석, 검증)\n",
    "    \n",
    "    # 모델 로딩 완료 및 각 GPU의 역할을 안내함\n",
    "    print(\"✅ 모델 로딩 완료!\")\n",
    "    print(\"   GPU 0: YOLOv8n (빠른 처리)\")    # 실시간성 우선 전략\n",
    "    print(\"   GPU 1: YOLOv8m (정확한 처리)\")  # 정확도 우선 전략\n",
    "    print()  # 빈 줄로 구분\n",
    "    \n",
    "    # 웹캠 준비 상태를 확인함\n",
    "    if webcam is None:\n",
    "        # 이전 단계에서 웹캠 초기화가 실패했다면 None 상태\n",
    "        print(\"❌ 웹캠이 준비되지 않았습니다!\")\n",
    "        return  # 웹캠 없이는 실시간 처리 불가능\n",
    "    \n",
    "    # 듀얼 GPU 실시간 처리 시작을 안내함\n",
    "    print(\"🎬 듀얼 GPU 실시간 처리 시작!\")\n",
    "    # 🎬 영화 카메라로 실시간 영상 처리를 표현함\n",
    "    print(\"💡 두 모델의 결과를 동시에 비교해볼 수 있습니다.\")\n",
    "    # 💡 전구로 핵심 기능임을 강조함\n",
    "    print(\"   's' 키: 스크린샷, 'q' 키: 종료\")\n",
    "    print()\n",
    "    \n",
    "    # 성능 측정을 위한 변수들을 초기화함\n",
    "    frame_count = 0              # 처리한 총 프레임 수를 카운트함\n",
    "    start_time = time.time()     # 시작 시간을 기록함 (FPS 계산용)\n",
    "    \n",
    "    try:  # 실시간 처리 중 발생할 수 있는 모든 오류에 대비함\n",
    "        # GPU 오류, 메모리 부족, 키보드 인터럽트 등에 안전하게 대응함\n",
    "        \n",
    "        while True:  # 실시간 처리를 위한 무한 루프\n",
    "            # 연속적인 프레임 처리로 부드러운 실시간 영상을 만듦\n",
    "            \n",
    "            # 웹캠에서 한 프레임을 읽어옴\n",
    "            ret, frame = webcam.read()\n",
    "            # ret: 프레임 읽기 성공 여부 (True/False)\n",
    "            # frame: 실제 이미지 데이터 (numpy 배열)\n",
    "            \n",
    "            if not ret:  # 프레임 읽기에 실패한 경우\n",
    "                break    # while 루프를 종료함\n",
    "            \n",
    "            # *** 핵심: 두 GPU에서 동시에 같은 프레임을 처리함 ***\n",
    "            results_gpu0 = model_gpu0(frame, device='cuda:0', verbose=False)  # GPU 0에서 빠른 처리\n",
    "            results_gpu1 = model_gpu1(frame, device='cuda:1', verbose=False)  # GPU 1에서 정확한 처리\n",
    "            # 이 두 줄이 동시에(병렬로) 실행됨!\n",
    "            # GPU 0과 GPU 1이 각각 독립적으로 같은 이미지를 분석함\n",
    "            # 병렬 처리로 전체 처리 시간이 단축됨 (직렬 처리 대비 약 2배 빠름)\n",
    "            # verbose=False: 로그 출력을 비활성화해서 성능을 최적화함\n",
    "            \n",
    "            # 각 GPU의 처리 결과를 시각화된 이미지로 변환함\n",
    "            frame_gpu0 = results_gpu0[0].plot()  # GPU 0 결과 (nano 모델의 검출 결과)\n",
    "            frame_gpu1 = results_gpu1[0].plot()  # GPU 1 결과 (medium 모델의 검출 결과)\n",
    "            # .plot(): 원본 이미지에 바운딩 박스와 라벨을 그린 새로운 이미지를 생성함\n",
    "            # 같은 이미지지만 서로 다른 모델로 처리한 결과를 비교할 수 있음\n",
    "            \n",
    "            # 두 결과 이미지를 좌우로 나란히 배치하여 하나의 화면을 생성함\n",
    "            combined_frame = np.hstack((frame_gpu0, frame_gpu1))\n",
    "            # np.hstack = numpy horizontal stack (수평 결합)\n",
    "            # 두 개의 이미지를 가로로 이어붙여서 하나의 큰 이미지를 만듦\n",
    "            # 좌측: GPU 0 결과, 우측: GPU 1 결과\n",
    "            \n",
    "            # 각 화면 영역에 라벨 텍스트를 추가함 (어떤 GPU/모델인지 구분)\n",
    "            cv2.putText(combined_frame, 'GPU 0: YOLOv8n (Fast)', (10, 30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            # 좌측 화면 상단에 GPU 0 정보를 표시함\n",
    "            # (10, 30): 좌측 상단 위치, 1: 폰트 크기, (0, 255, 0): 녹색\n",
    "            \n",
    "            cv2.putText(combined_frame, 'GPU 1: YOLOv8m (Accurate)', (frame.shape[1] + 10, 30), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "            # 우측 화면 상단에 GPU 1 정보를 표시함\n",
    "            # frame.shape[1]: 원본 프레임의 너비만큼 x좌표를 이동해서 우측 영역에 배치\n",
    "            # +10: 약간의 여백을 추가함\n",
    "            \n",
    "            # 전체 시스템의 FPS를 표시함 (듀얼 GPU 통합 성능)\n",
    "            frame_count += 1  # 성공적으로 처리된 프레임 수를 1 증가시킴\n",
    "            fps = frame_count / (time.time() - start_time)  # 평균 FPS를 계산함\n",
    "            # FPS = 총 프레임 수 ÷ 경과 시간\n",
    "            cv2.putText(combined_frame, f'FPS: {fps:.1f}', (10, 70), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2)\n",
    "            # (255, 255, 0): 노란색으로 FPS 정보를 표시함 (눈에 잘 띄게)\n",
    "            # 듀얼 GPU의 전체적인 처리 성능을 실시간으로 모니터링할 수 있음\n",
    "            \n",
    "            # 완성된 듀얼 화면을 모니터에 출력함\n",
    "            cv2.imshow('Dual GPU YOLO Comparison', combined_frame)\n",
    "            # 창 제목으로 듀얼 GPU 비교임을 명시함\n",
    "            # combined_frame: 두 GPU 결과가 합쳐진 최종 이미지\n",
    "            \n",
    "            # 키보드 입력을 확인함 (1ms 대기로 실시간성 유지)\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "            # 1ms: 실시간 처리에 방해되지 않는 최소한의 대기 시간\n",
    "            \n",
    "            if key == ord('q'):  # 'q' 키: 프로그램 종료\n",
    "                print(\"👋 듀얼 GPU 처리를 종료합니다!\")\n",
    "                break  # while 루프 종료\n",
    "                \n",
    "            elif key == ord('s'):  # 's' 키: 스크린샷 저장\n",
    "                # 현재 듀얼 GPU 결과 화면을 이미지 파일로 저장함\n",
    "                filename = f'dual_gpu_screenshot_{int(time.time())}.jpg'\n",
    "                # 파일명에 타임스탬프를 포함해서 고유성을 보장함\n",
    "                cv2.imwrite(filename, combined_frame)  # 전체 듀얼 화면을 저장함\n",
    "                print(f\"📸 듀얼 GPU 스크린샷 저장: {filename}\")\n",
    "                # 📸 카메라 이모지로 저장 완료를 알림\n",
    "    \n",
    "    except KeyboardInterrupt:  # Ctrl+C로 강제 종료했을 때\n",
    "        print(\"\\n⚠️  사용자에 의해 중단됨\")\n",
    "        # ⚠️ 경고 표시로 의도적 중단임을 알림\n",
    "    \n",
    "    finally:  # 정상 종료든 오류 종료든 반드시 실행되는 정리 코드\n",
    "        cv2.destroyAllWindows()  # 모든 OpenCV 창을 닫고 메모리에서 제거함\n",
    "        # GPU 메모리와 GUI 자원을 해제해서 시스템 안정성을 확보함\n",
    "        \n",
    "        # 최종 성능 통계를 출력함\n",
    "        total_time = time.time() - start_time  # 총 실행 시간\n",
    "        avg_fps = frame_count / total_time if total_time > 0 else 0  # 전체 평균 FPS\n",
    "        print(f\"🏆 듀얼 GPU 평균 FPS: {avg_fps:.2f}\")\n",
    "        # 🏆 트로피로 최종 성과를 강조함\n",
    "        # 듀얼 GPU의 실제 성능 향상 정도를 정량적으로 확인할 수 있음\n",
    "\n",
    "# 듀얼 GPU 데모 함수를 실행함\n",
    "print(\"💡 듀얼 GPU 처리 조작법:\")\n",
    "print(\"   🔤 키보드 조작:\")\n",
    "print(\"   - 'q': 종료\")\n",
    "print(\"   - 's': 듀얼 GPU 결과 스크린샷 저장\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "dual_gpu_demonstration()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)  # 다음 단계와 구분하는 마무리 선\n",
    "\n",
    "# 💡 이 코드에서 배우는 혁신적인 고급 개념들:\n",
    "# 1. 진정한 병렬 처리 - 두 GPU가 동시에 독립적으로 작업을 수행하는 멀티태스킹\n",
    "# 2. 실시간 성능 비교 - 빠름 vs 정확함의 트레이드오프를 시각적으로 직접 확인\n",
    "# 3. 하드웨어 최적화 - 각 GPU의 특성과 용도에 맞는 최적 모델을 배치하는 전략\n",
    "# 4. 시각적 통합 - 서로 다른 결과를 하나의 화면에 자연스럽게 결합하는 UI 설계\n",
    "# 5. 확장성 - 더 많은 GPU로 확장 가능한 구조적 설계 (3개, 4개 GPU로 확장 가능)\n",
    "# 6. 리소스 관리 - 각 GPU의 메모리와 연산 능력을 효율적으로 분배하는 방법\n",
    "# 7. 실시간 모니터링 - 멀티 GPU 시스템의 전체 성능을 실시간으로 추적하는 기법\n",
    "#\n",
    "# 🔥 이는 실제 AI 연구소나 대기업에서 사용하는 최고급 기술임!\n",
    "# 🏢 Google, OpenAI, Tesla 등에서 실제로 사용하는 분산 처리 기법\n",
    "# 🎯 여러분은 이제 멀티 GPU 시스템을 다룰 수 있는 전문가 수준에 도달함!\n",
    "# 💰 실무에서 이런 기술은 연봉을 크게 좌우하는 핵심 경쟁력\n",
    "# 🚀 클라우드 컴퓨팅, 슈퍼컴퓨터 활용 능력의 기초가 되는 지식\n",
    "# 🔬 AI 연구개발에서 필수적인 대용량 데이터 처리 능력\n",
    "# 🎮 게임 개발, VR/AR, 메타버스 등 고성능이 필요한 모든 분야에 응용 가능!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff473a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "마지막 단계: 사용한 리소스들을 정리하고 마무리합니다.\n",
    "\"\"\"\n",
    "\n",
    "print(\"🧹 리소스 정리 및 마무리\")\n",
    "print(\"=\"*24)\n",
    "\n",
    "# 웹캠 해제\n",
    "if webcam is not None:\n",
    "    webcam.release()\n",
    "    print(\"✅ 웹캠 해제 완료\")\n",
    "\n",
    "# 모든 OpenCV 윈도우 닫기\n",
    "cv2.destroyAllWindows()\n",
    "print(\"✅ 모든 윈도우 닫기 완료\")\n",
    "\n",
    "# GPU 메모리 정리\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"✅ GPU 메모리 정리 완료\")\n",
    "\n",
    "print(\"\\n🎉 YOLO 실시간 객체인식 체험 완료!\")\n",
    "print(\"=\"*35)\n",
    "\n",
    "print(\"📚 오늘 배운 내용:\")\n",
    "print(\"   1. ✅ AI와 컴퓨터 비전의 기본 개념\")\n",
    "print(\"   2. ✅ YOLO 알고리즘의 동작 원리\")\n",
    "print(\"   3. ✅ OpenCV와 YOLO의 협업 관계\")\n",
    "print(\"   4. ✅ GPU 가속을 통한 고성능 처리\")\n",
    "print(\"   5. ✅ 실시간 객체 인식 구현\")\n",
    "print(\"   6. ✅ 다양한 모델 성능 비교\")\n",
    "print(\"   7. ✅ 고급 기능 (신뢰도 조절, 필터링)\")\n",
    "print(\"   8. ✅ 듀얼 GPU 활용 (해당되는 경우)\")\n",
    "\n",
    "print(\"\\n🚀 더 나아가기:\")\n",
    "print(\"   • 커스텀 데이터셋으로 모델 훈련하기\")\n",
    "print(\"   • 다른 AI 비전 기술 탐구 (세그멘테이션, 포즈 추정)\")\n",
    "print(\"   • 실제 프로젝트에 응용하기\")\n",
    "print(\"   • AI 윤리와 프라이버시 고려사항 학습\")\n",
    "\n",
    "print(\"\\n💡 집에서 해볼 수 있는 과제:\")\n",
    "print(\"   1. 다른 YOLO 모델들 (YOLOv9, YOLOv10) 시도해보기\")\n",
    "print(\"   2. 특정 객체만 카운팅하는 프로그램 만들기\")\n",
    "print(\"   3. 비디오 파일로 객체 인식 해보기\")\n",
    "print(\"   4. 인식 결과를 파일로 저장하기\")\n",
    "\n",
    "print(\"\\n🔗 유용한 참고 자료:\")\n",
    "print(\"   • Ultralytics 공식 문서: https://docs.ultralytics.com/\")\n",
    "print(\"   • YOLO 논문: https://arxiv.org/abs/1506.02640\")\n",
    "print(\"   • OpenCV 튜토리얼: https://opencv.org/\")\n",
    "print(\"   • PyTorch 공식 사이트: https://pytorch.org/\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🎯 수고하셨습니다! AI의 놀라운 세계에 오신 것을 환영합니다!\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5b2f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "보너스: 자주 발생하는 문제들과 해결 방법\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n🛠️  문제 해결 가이드\")\n",
    "print(\"=\"*19)\n",
    "\n",
    "troubleshooting_guide = {\n",
    "    \"ModuleNotFoundError\": {\n",
    "        \"문제\": \"필요한 패키지가 설치되지 않음\",\n",
    "        \"해결방법\": [\n",
    "            \"pip install ultralytics torch opencv-python\",\n",
    "            \"가상환경 활성화 확인\",\n",
    "            \"Python 버전 확인 (3.8-3.11 권장)\"\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    \"CUDA out of memory\": {\n",
    "        \"문제\": \"GPU 메모리 부족\",\n",
    "        \"해결방법\": [\n",
    "            \"torch.cuda.empty_cache() 실행\",\n",
    "            \"더 작은 모델 사용 (yolov8n)\",\n",
    "            \"배치 크기 줄이기\",\n",
    "            \"다른 프로그램 종료\"\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    \"웹캠이 열리지 않음\": {\n",
    "        \"문제\": \"cv2.VideoCapture(0) 실패\",\n",
    "        \"해결방법\": [\n",
    "            \"다른 번호 시도 (1, 2, 3...)\",\n",
    "            \"다른 프로그램에서 웹캠 사용 중인지 확인\",\n",
    "            \"웹캠 드라이버 업데이트\",\n",
    "            \"관리자 권한으로 실행\"\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    \"느린 처리 속도\": {\n",
    "        \"문제\": \"FPS가 너무 낮음\",\n",
    "        \"해결방법\": [\n",
    "            \"GPU 사용 확인 (cuda vs cpu)\",\n",
    "            \"더 작은 모델 사용\",\n",
    "            \"해상도 줄이기\",\n",
    "            \"신뢰도 임계값 높이기\"\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "    \"인식 정확도가 낮음\": {\n",
    "        \"문제\": \"객체를 잘못 인식하거나 놓침\",\n",
    "        \"해결방법\": [\n",
    "            \"더 큰 모델 사용 (yolov8m, yolov8l)\",\n",
    "            \"신뢰도 임계값 낮추기\",\n",
    "            \"조명 개선\",\n",
    "            \"카메라 각도 조정\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "for problem, details in troubleshooting_guide.items():\n",
    "    print(f\"\\n❌ {problem}\")\n",
    "    print(f\"   🔍 {details['문제']}\")\n",
    "    print(\"   💡 해결방법:\")\n",
    "    for i, solution in enumerate(details['해결방법'], 1):\n",
    "        print(f\"      {i}. {solution}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e65bb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 추가 실험 아이디어들\n",
    "# =============================================================================\n",
    "\n",
    "\"\"\"\n",
    "더 궁금한 학생들을 위한 추가 실험 아이디어들\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n🧪 추가 실험 아이디어\")\n",
    "print(\"=\"*20)\n",
    "\n",
    "experiment_ideas = [\n",
    "    {\n",
    "        \"제목\": \"객체 카운터 만들기\",\n",
    "        \"설명\": \"특정 객체(예: 사람)의 개수를 실시간으로 세는 프로그램\",\n",
    "        \"난이도\": \"⭐⭐☆☆☆\",\n",
    "        \"키워드\": \"카운팅, 통계, 데이터 수집\"\n",
    "    },\n",
    "    {\n",
    "        \"제목\": \"움직임 감지 + 객체 인식\",\n",
    "        \"설명\": \"움직이는 객체만 인식하여 보안 시스템 만들기\",\n",
    "        \"난이도\": \"⭐⭐⭐☆☆\",\n",
    "        \"키워드\": \"모션 디텍션, 보안, 배경 차분\"\n",
    "    },\n",
    "    {\n",
    "        \"제목\": \"실시간 객체 추적\",\n",
    "        \"설명\": \"인식된 객체를 시간에 따라 추적하기\",\n",
    "        \"난이도\": \"⭐⭐⭐⭐☆\",\n",
    "        \"키워드\": \"트래킹, 경로 분석, ID 할당\"\n",
    "    },\n",
    "    {\n",
    "        \"제목\": \"다중 카메라 시스템\",\n",
    "        \"설명\": \"여러 웹캠을 동시에 사용한 광역 모니터링\",\n",
    "        \"난이도\": \"⭐⭐⭐⭐⭐\",\n",
    "        \"키워드\": \"멀티 스트림, 동기화, 통합 관제\"\n",
    "    },\n",
    "    {\n",
    "        \"제목\": \"음성 알림 시스템\",\n",
    "        \"설명\": \"특정 객체 감지 시 음성으로 알림\",\n",
    "        \"난이도\": \"⭐⭐⭐☆☆\",\n",
    "        \"키워드\": \"TTS, 알림, 이벤트 처리\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, idea in enumerate(experiment_ideas, 1):\n",
    "    print(f\"\\n💡 실험 {i}: {idea['제목']}\")\n",
    "    print(f\"   📝 {idea['설명']}\")\n",
    "    print(f\"   📊 난이도: {idea['난이도']}\")\n",
    "    print(f\"   🏷️  관련 키워드: {idea['키워드']}\")\n",
    "\n",
    "print(\"\\n🎓 이런 실험들을 통해 AI 개발자의 꿈을 키워보세요!\")\n",
    "print(\"질문이 있으면 언제든 선생님께 물어보세요! 😊\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🏁 모든 실습이 완료되었습니다! 수고하셨습니다! 🏁\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
